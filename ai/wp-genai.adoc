---
sidebar: sidebar
permalink: ai/wp-genai.html
keywords: NetApp AI, AI, Artificial Intelligence, Generative AI
summary: NetApp® AI capabilities that enable seamless data management and data movement across the AI pipeline for training, retraining, fine tuning, inferencing, and monitoring generative AI models. 
---

= Generative AI and NetApp Value
:hardbreaks:
:nofooter:
:icons: font
:linkattrs:
:imagesdir: ./../media/

Sathish Thyagarajan, NetApp

[.lead]
The demand for generative artificial intelligence (AI) is driving disruption across industries. Many organizations are using generative AI to build new features and prototype AI powered applications that deliver better results and consumer experiences. Generative AI such as Generative Pre-trained Transformers (GPT) use neural networks to create new content, as diverse as text, audio, video, code, images, and proteins. It is crucial to design a robust AI infrastructure and a free-flowing data pipeline that takes advantage of the compelling data storage and data management features of on-premises, hybrid and multicloud deployment options that can reduce risks associated with data quality, data governance and data security before companies can design AI solutions with large language models (LLMs). This paper describes these considerations and the corresponding NetApp® AI capabilities that enable seamless data management and data movement across the AI pipeline for training, retraining, fine tuning, inferencing, and monitoring generative AI models. 

== Executive Summary 
Most recently, new AI tools used to generate text, code, image, or therapeutic proteins in response to user prompts have gained significant fame. This indicates users can make a request using natural language and AI will interpret and generate text, such as news articles or product descriptions that reflect user request or produce code, music, speech, visual effects, and 3D assets using algorithms trained on already existing data. As a result, phrases like Stable Diffusion, Hallucinations, Prompt Engineering and Value Alignment are rapidly emerging in the design of AI systems. These self-supervised or semi-supervised machine learning (ML) models are becoming widely available as pre-trained foundation models (FM) via cloud service providers and other AI firmsvendors, which are being adopted by various business establishments across industries for a wide range of downstream NLP (natural language processing) tasks. As asserted by research analyst firms like McKinsey – “Generative AI’s impact on productivity could add trillions of dollars in value to the global economy.” While companies are reimagining AI as thought partners to humans and FMs are broadening simultaneously to what businesses and institutions can do with generative AI, the opportunities to manage massive volumes of data will continue to grow. This document presents introductory information on generative AI and the design concepts in relation to NetApp capabilities that bring value to NetApp customers, both on-premises and hybrid or multicloud environments. 

*So, what’s in it for NetApp customers to use NetApp in their AI environments?* NetApp helps organizations meet the complexities created by rapid data and cloud growth, multi-cloud management, and the adoption of next-generation technologies, such as AI. NetApp has combined various capabilities into intelligent data management software and storage infrastructure that have been exceptionally designedwell balanced with high-performance optimized for AI workloads. Generative AI solutions like LLMs need to read and process their source datasets from storage into memory numerous times to foster intelligence. NetApp has been a leader in data movementmobility, data governance and data security technologies across the edge-to-core-to-cloud ecosystem, serving enterprise customers to build at-scale AI solutions. NetApp, with a strong network of partners has been helping chief data officers, AI engineers and data scientists in the design of a free-flowing data pipeline for data preparation, data protection, and strategic data management responsibilities of AI model training and inferencing, optimizing the performance and scalability of the AI/ML lifecycle. NetApp data technologies and capabilities such as NetApp® ONTAP AI®, BlueXP®, SnapMirror®, FlexCache®, NetApp® Data Ops Toolkit, ONTAP support for NVIDIA® AI Enterprise and NVIDIA GPU Direct Storage, accelerate the data science pipeline and streamline the development and deployment of production AI systems including unsupervised generative AI models like the transformer based LLMs. As enterprises of all types embrace new AI tools, they face data challenges from the edge to the data center to the cloud that demand for scalable, responsible and explainable AI solutions.​ As the data authority on hybrid and multi cloud, NetApp is building a network of partners and joint solutions that can help with all aspects of constructing a data pipeline (and data lakes) for generative AI model training, model fine-tuning, context-based inferencing and model monitoring. 

== What is Generative AI? 
Generative AI is changing how we create content. It illustrates neural network frameworks like Generative Adversarial Network (GAN), Variational Autoencoders (VAE), and Generative Pre-Trained Transformers (GPT), which can generate new content like text, code, audio, video, and synthetic data. Digital artists are starting to apply a combination of rendering technologies like NeRF (Neural Radiance Field) with generative AI to convert static 2D images into immersive 3D scenes. Transformer-based models like OpenAI’s Chat-GPT, Google’s Bard and Meta’s LLaMA have emerged as the foundational technology underpinning many advances in large language models. These LLMs are often characterized by four parameters: (1) Size of the model; (2) Size of the training dataset; (3) Cost of training, and (4) Model performance after training. LLMs also fall mainly into three transformer architectures: 

Encoder-only models: Generate embeddings that can be further used for classification tasks.  

Eg: BERT (Google, 2018), ALBERT (Google, 2020), RoBERTa (Meta, 2019)    

Encoder – Decoder models: Generative tasks where output heavily relies on input (translation, summarization). Eg: BART (Meta, 2020), T5 (Google, 2022),  

Decoder-only models: Generative tasks such as story writing, blog generation etc.  

Eg: LaMDA (Google, 2021), GPT-n (OpenAI, 2018 – 2023), LLaMA (Meta, 2023), PaLM-E (Google, 2023) 

=== Enterprise Use Cases and Downstream NLP Tasks 
Businesses across industries are uncovering even greater potential for AI to extract and produce new forms of value from existing data for business operations, sales, marketing, and legal services. According to IDC (International Data Corporation) market intelligence on global generative AI use cases and investments, knowledge management in software development and product design is to be the most impacted, followed by storyline creation for marketing and code generation for developers. In healthcare, clinical research organizations are breaking new ground in medicine. Pretrained models like ProteinBERT incorporate Gene Ontology (GO) annotations to rapidly design proteins for medical drugs, representing a significant milestone in drug discovery, bioinformatics, and molecular biology. Biotech firms have initiated human trials for generative AI-discovered medicine, that aims to treat diseases like pulmonary fibrosis (IPF), a lung disease that causes irreversible scarring of lung tissue. 

image:ai-genai-image2.png

=== Foundation Models 
A foundation model (FM) also known as base model is a large AI model trained on vast quantities of unlabeled data, using self-supervision at scale, generally adapted for a wide range of downstream tasks. Since the training data is not labelled by humans, the model emerges rather than being explicitly encoded. This means the model can generate stories or a narrative of its own without being explicitly programmed to do so. Another important characteristic of FM is homogenization, which means the same method is used in many domains. However, with personalization and fine-tuning techniques, FMs integrated into products appearing these days are not only good at generating text but also for explaining and debugging code. Like for example, GitHub Copilot leverages OpenAI Codex to assist developers in writing code.  

=== Fine-tuning, domain-specificity, and retraining 
One of the common practices following data preparation and data pre-processing is to select a pre-trained model that has been trained on a large and diverse dataset. This can be a large language model such as BERT (Bidirectional Encoder Representation from Transformers) or those available through other AI vendors. Once the pre-trained model is selected, the next step is to fine-tune it on the domain-specific data. This involves adjusting the model’s parameters and training it on the new data to adapt to the specific domain and task. Domain-specific models designed and trained for a specific task generally have higher accuracy and performance within their scope, but low transferability across other tasks or domains. As business environment and data change over a period, the prediction accuracy of FM models could also begin to decline compared to their performance during testing. This is when retraining the FM model becomes crucial. Model retraining refers to updating a deployed machine learning model with new data, generally performed to eliminate two types of drifts that occur. (1) Concept drift – when the link between the input variables and the target variables changes over time. Since the description of what we want to predict changes, the model can provide inaccurate predictions. (2) Data drift – occurs when the characteristics of the input data change. Like changes in customer habits or behavior over time and the model’s inability to respond to such change. 

=== Prompt engineering and Inferencing 
Prompt engineering refers to the effective methods of how to communicate with LLMs to perform desired tasks without updating the model weights. As important as AI model training is to NLP applications, inferencing is equally important, where the trained models respond to user prompts. The system requirements for inferencing are much more on the read performance of the AI storage system as it needs to be able to apply billions of stored model parameters to produce the best response. 

=== LLMOps, Model Monitoring and Vectorstores 
Like traditional Machine Learning Ops (MLOps), Large Language Model Ops (LLMOps) require the collaboration of data scientists and DevOps engineers with tools and best practices for the management of LLMs in production environments, even though, the workflow and tech stack for LLMs could vary in some ways. For instance, LLM pipelines, built using frameworks like LangChain string together multiple LLM API calls to external embedding endpoints such as vectorstores or vector databases. The use of an embedding endpoint and vectorstore for downstream connectors (like to a vector database) represents a significant development in how data is stored and accessed. As opposed to traditional ML models that are developed from scratch, LLMs often rely on transfer learning since these models start with FMs that are fine-tuned with new data to improve performance in a more specific domain. Therefore, it is crucial that an enterprise MLOps platform supports the data science at-scale requirements of LLMOps with capabilities of model monitoring and risk management. 

=== Risks and Ethics in the age of Generative AI 
Garbage in – garbage out, has always been the challenging case with computing. The only difference with generative AI is that it excels at making the garbage highly credible. Therefore, companies that see generative AI as a great opportunity to lower their costs with AI equivalents need to efficiently detect deep fakes, lower risks, and reduce biases, to keep the systems honest and ethical. A free-flowing data pipeline with the capabilities of data quality, data governance and data security are eminent for the design of AI applications with LLM models. 

== NetApp capabilities
The question of whether to adopt a pre-trained model or design a custom solution leveraging open-source frameworks is a crucial strategic decision. Apart from the organization’s selection of LLM deployment option, i.e training a large model from scratch versus retraining or fine-tuning a pre-trained LLM model, the workflow of an LLM lifecycle typically follows that of a traditional ML workflow. The movement and management of data in generative AI applications such as chatbot, code generation, genome model expression, or image generation, can span across the edge, private data center, hybrid and multicloud ecosystem. A data pipeline with strategic data management capabilities is critical to AI operations so that appropriate resources are used for generative AI datasets along the LLM workflow. 



