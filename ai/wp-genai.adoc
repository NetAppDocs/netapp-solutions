---
sidebar: sidebar
permalink: ai/wp-genai.html
keywords: NetApp AI, AI, Artificial Intelligence, Generative AI
summary: NetApp® AI capabilities that enable seamless data management and data movement across the AI pipeline for training, retraining, fine tuning, inferencing, and monitoring generative AI models. 
---

= Generative AI and NetApp Value
:hardbreaks:
:nofooter:
:icons: font
:linkattrs:
:imagesdir: ./../media/
Data Management in Large Language Models
Sathish Thyagarajan, NetApp

[.lead]
Abstract
The demand for generative artificial intelligence (AI) is driving disruption across industries, enhancing business creativity and product innovation. Many organizations are using generative AI to build new product features, improve engineering productivity and prototype AI powered applications that deliver better results and consumer experiences. Generative AI such as Generative Pre-trained Transformers (GPT) use neural networks to create new content, as diverse as text, audio, and video. Given the extreme scale and massive datasets involved with large language models (LLMs), it is crucial to architect a robust AI infrastructure that takes advantage of the compelling data storage features of on-premises, hybrid and multicloud deployment options and reduce risks associated with data mobility, data protection and governance before companies can design AI solutions. This paper describes these considerations and the corresponding NetApp® AI capabilities that enable seamless data management and data movement across the AI data pipeline for training, retraining, fine-tuning, and inferencing generative AI models.

== Executive Summary 
Most recently, new AI tools used to generate text, code, image, or therapeutic proteins in response to user prompts have gained significant fame. This indicates users can make a request using natural language and AI will interpret and generate text, such as news articles or product descriptions that reflect user request or produce code, music, speech, visual effects, and 3D assets using algorithms trained on already existing data. As a result, phrases like Stable Diffusion, Hallucinations, Prompt Engineering and Value Alignment are rapidly emerging in the design of AI systems. These self-supervised or semi-supervised machine learning (ML) models are becoming widely available as pre-trained foundation models (FM) via cloud service providers and other AI firmsvendors, which are being adopted by various business establishments across industries for a wide range of downstream NLP (natural language processing) tasks. As asserted by research analyst firms like McKinsey – “Generative AI’s impact on productivity could add trillions of dollars in value to the global economy.” While companies are reimagining AI as thought partners to humans and FMs are broadening simultaneously to what businesses and institutions can do with generative AI, the opportunities to manage massive volumes of data will continue to grow. This document presents introductory information on generative AI and the design concepts in relation to NetApp capabilities that bring value to NetApp customers, both on-premises and hybrid or multicloud environments. 

*So, what’s in it for NetApp customers to use NetApp in their AI environments?* NetApp helps organizations meet the complexities created by rapid data and cloud growth, multi-cloud management, and the adoption of next-generation technologies, such as AI. NetApp has combined various capabilities into intelligent data management software and storage infrastructure that have been exceptionally designedwell balanced with high-performance optimized for AI workloads. Generative AI solutions like LLMs need to read and process their source datasets from storage into memory numerous times to foster intelligence. NetApp has been a leader in data movementmobility, data governance and data security technologies across the edge-to-core-to-cloud ecosystem, serving enterprise customers to build at-scale AI solutions. NetApp, with a strong network of partners has been helping chief data officers, AI engineers and data scientists in the design of a free-flowing data pipeline for data preparation, data protection, and strategic data management responsibilities of AI model training and inferencing, optimizing the performance and scalability of the AI/ML lifecycle. NetApp data technologies and capabilities such as NetApp® ONTAP AI®, BlueXP®, SnapMirror®, FlexCache®, NetApp® Data Ops Toolkit, ONTAP support for NVIDIA® AI Enterprise and NVIDIA GPU Direct Storage, accelerate the data science pipeline and streamline the development and deployment of production AI systems including unsupervised generative AI models like the transformer based LLMs. As enterprises of all types embrace new AI tools, they face data challenges from the edge to the data center to the cloud that demand for scalable, responsible and explainable AI solutions.​ As the data authority on hybrid and multi cloud, NetApp is building a network of partners and joint solutions that can help with all aspects of constructing a data pipeline (and data lakes) for generative AI model training, model fine-tuning, context-based inferencing and model monitoring. 

== What is Generative AI? 
Generative AI is changing how we create content. It illustrates neural network frameworks like Generative Adversarial Network (GAN), Variational Autoencoders (VAE), and Generative Pre-Trained Transformers (GPT), which can generate new content like text, code, audio, video, and synthetic data. Digital artists are starting to apply a combination of rendering technologies like NeRF (Neural Radiance Field) with generative AI to convert static 2D images into immersive 3D scenes. Transformer-based models like OpenAI’s Chat-GPT, Google’s Bard and Meta’s LLaMA have emerged as the foundational technology underpinning many advances in large language models. These LLMs are often characterized by four parameters: (1) Size of the model; (2) Size of the training dataset; (3) Cost of training, and (4) Model performance after training. LLMs also fall mainly into three transformer architectures: 

Encoder-only models: Generate embeddings that can be further used for classification tasks.  

Eg: BERT (Google, 2018), ALBERT (Google, 2020), RoBERTa (Meta, 2019)    

Encoder – Decoder models: Generative tasks where output heavily relies on input (translation, summarization). Eg: BART (Meta, 2020), T5 (Google, 2022),  

Decoder-only models: Generative tasks such as story writing, blog generation etc.  

Eg: LaMDA (Google, 2021), GPT-n (OpenAI, 2018 – 2023), LLaMA (Meta, 2023), PaLM-E (Google, 2023) 

=== Enterprise Use Cases and Downstream NLP Tasks 
Businesses across industries are uncovering even greater potential for AI to extract and produce new forms of value from existing data for business operations, sales, marketing, and legal services. According to IDC (International Data Corporation) market intelligence on global generative AI use cases and investments, knowledge management in software development and product design is to be the most impacted, followed by storyline creation for marketing and code generation for developers. In healthcare, clinical research organizations are breaking new ground in medicine. Pretrained models like ProteinBERT incorporate Gene Ontology (GO) annotations to rapidly design proteins for medical drugs, representing a significant milestone in drug discovery, bioinformatics, and molecular biology. Biotech firms have initiated human trials for generative AI-discovered medicine, that aims to treat diseases like pulmonary fibrosis (IPF), a lung disease that causes irreversible scarring of lung tissue. 

image:ai-genai-image2.png

=== Foundation Models 
A foundation model (FM) also known as base model is a large AI model trained on vast quantities of unlabeled data, using self-supervision at scale, generally adapted for a wide range of downstream tasks. Since the training data is not labelled by humans, the model emerges rather than being explicitly encoded. This means the model can generate stories or a narrative of its own without being explicitly programmed to do so. Another important characteristic of FM is homogenization, which means the same method is used in many domains. However, with personalization and fine-tuning techniques, FMs integrated into products appearing these days are not only good at generating text but also for explaining and debugging code. Like for example, GitHub Copilot leverages OpenAI Codex to assist developers in writing code.  

=== Fine-tuning, domain-specificity, and retraining 
One of the common practices following data preparation and data pre-processing is to select a pre-trained model that has been trained on a large and diverse dataset. This can be a large language model such as BERT (Bidirectional Encoder Representation from Transformers) or those available through other AI vendors. Once the pre-trained model is selected, the next step is to fine-tune it on the domain-specific data. This involves adjusting the model’s parameters and training it on the new data to adapt to the specific domain and task. Domain-specific models designed and trained for a specific task generally have higher accuracy and performance within their scope, but low transferability across other tasks or domains. As business environment and data change over a period, the prediction accuracy of FM models could also begin to decline compared to their performance during testing. This is when retraining the FM model becomes crucial. Model retraining refers to updating a deployed machine learning model with new data, generally performed to eliminate two types of drifts that occur. (1) Concept drift – when the link between the input variables and the target variables changes over time. Since the description of what we want to predict changes, the model can provide inaccurate predictions. (2) Data drift – occurs when the characteristics of the input data change. Like changes in customer habits or behavior over time and the model’s inability to respond to such change. 

=== Prompt engineering and Inferencing 
Prompt engineering refers to the effective methods of how to communicate with LLMs to perform desired tasks without updating the model weights. As important as AI model training is to NLP applications, inferencing is equally important, where the trained models respond to user prompts. The system requirements for inferencing are much more on the read performance of the AI storage system as it needs to be able to apply billions of stored model parameters to produce the best response. 

=== LLMOps, Model Monitoring and Vectorstores 
Like traditional Machine Learning Ops (MLOps), Large Language Model Ops (LLMOps) require the collaboration of data scientists and DevOps engineers with tools and best practices for the management of LLMs in production environments, even though, the workflow and tech stack for LLMs could vary in some ways. For instance, LLM pipelines, built using frameworks like LangChain string together multiple LLM API calls to external embedding endpoints such as vectorstores or vector databases. The use of an embedding endpoint and vectorstore for downstream connectors (like to a vector database) represents a significant development in how data is stored and accessed. As opposed to traditional ML models that are developed from scratch, LLMs often rely on transfer learning since these models start with FMs that are fine-tuned with new data to improve performance in a more specific domain. Therefore, it is crucial that an enterprise MLOps platform supports the data science at-scale requirements of LLMOps with capabilities of model monitoring and risk management. 

=== Risks and Ethics in the age of Generative AI 
Garbage in – garbage out, has always been the challenging case with computing. The only difference with generative AI is that it excels at making the garbage highly credible. Therefore, companies that see generative AI as a great opportunity to lower their costs with AI equivalents need to efficiently detect deep fakes, lower risks, and reduce biases, to keep the systems honest and ethical. A free-flowing data pipeline with the capabilities of data quality, data governance and data security are eminent for the design of AI applications with LLM models. 

== NetApp capabilities
The question of whether to adopt a pre-trained model or design a custom solution leveraging open-source frameworks is a crucial strategic decision. Apart from the organization’s selection of LLM deployment option, i.e training a large model from scratch versus retraining or fine-tuning a pre-trained LLM model, the workflow of an LLM lifecycle typically follows that of a traditional ML workflow. The movement and management of data in generative AI applications such as chatbot, code generation, genome model expression, or image generation, can span across the edge, private data center, hybrid and multicloud ecosystem. A data pipeline with strategic data management capabilities is critical to AI operations so that appropriate resources are used for generative AI datasets along the LLM workflow. 

NetApp's portfolio of cloud services and storage infrastructure is powered by intelligent data management software. LLMs rely on large training datasets, which can introduce data risks from biases present in the data that can lead to unfair responses generated by the LLMs. NetApp minimizes the challenges by accelerating the AI training and retraining workflow, by making it easier for data scientists working with multiple copies of real-world or synthetic datasets for deploying generative AI applications. For instance, copying a 10TB dataset can take 2 seconds rather than hours. With NetApp these data copies are also stored efficiently. For example, data scientists can make 10 copies of each dataset with a reduction in storage space of up to 90%. 

Data Preparation: The first pillar of the LLM tech stack is largely untouched from the older traditional ML stack. Data preprocessing in AI pipeline is necessary to normalize and cleanse the data before training. This step includes connectors to ingest data wherever it may reside in the form of an Amazon S3 tier or in on-premises storage systems such as a file store or an object store like NetApp StorageGRID.  

*Storage infrastructure – NetApp All-Flash FAS (AFF-Series)* is a scale-out platform built for virtualized environments, combining low-latency performance via flash memory with best-in-class data management, built-in efficiencies, integrated data protection, multiprotocol support, and nondisruptive operations; cloud and on-premises. The AFF-series, powered by ONTAP, allows data scientists to connect to hybrid and multicloud deployments for more data services, data tiering, and caching. 

Intelligent data management software – NetApp ONTAP is the foundational technology that underpins NetApp's critical storage solutions in the data center and the cloud. ONTAP includes various data management and protection features and capabilities, including automatic ransomware protection against cyber-attacks, built-in data transport features, and storage efficiency capabilities. ONTAP provides the flexibility to design and deploy a storage environment across the broadest range of architectures – from on-premises, hybrid, public, and private clouds. It can be used in NAS, SAN, object environments, and software defined storage (SDS) situations.  

*ONTAP FabricPool*. This feature provides automatic tiering of cold data to public and private cloud storage options, including Amazon Web Services (AWS), Microsoft Azure, Google Cloud Storage and NetApp StorageGRID®. 

*NetApp FlexCache* is an ONTAP feature for remote caching capability that simplifies file distribution, reduces WAN latency, and lowers WAN bandwidth costs. Deploying NetApp FlexCache software allows data scientists and LLMOps or DevOps engineers to scale out storage performance for read-heavy workloads, such as AI inferencing of LLMs. 

NetApp ONTAP FlexGroup enables massive scalability in a single namespace to more than 20PB with over 400 billion files, while evenly spreading the performance across the cluster.  

*NetApp SnapMirror* is an ONTAP feature that replicates volume snapshots between any two ONTAP systems. This feature optimally transfers data at the edge to your on-premises data center or to the cloud. It efficiently transfers only changes, saving bandwidth and speeding replication.  

*NetApp File-Object Duality*. NetApp ONTAP enables dual-protocol access for NFS and S3. With this solution, data scientists and AI engineers can access NFS data from Amazon AWS SageMaker notebooks via S3 buckets from NetApp Cloud Volumes ONTAP. Likewise, NetApp CVS for Google cloud can benefit data scientists training or fine-tuning LLM models on Google VertexAI generative AI. This approach enables easy access and sharing of the same data from both NFS and S3 without the need for additional software and can be useful for the purpose of training or retraining LLMs. 

Cloud storage, data services, and software. The NetApp Cloud Volumes Platform is an integrated collection of cloud storage infrastructure and data services. The platform is anchored by NetApp Cloud Volumes ONTAP, a cloud-based software for customers who wish to manage their own cloud storage infrastructure. It is a managed, high-performance file system that enables you to run highly available AI workloads with improved data security in public clouds. It is based on the same ONTAP data management software that underpins our storage infrastructure offerings. Fully managed cloud storage offerings are available natively on Microsoft Azure as Azure NetApp Files, on AWS as Amazon FSx for NetApp ONTAP, and on Google Cloud as NetApp Cloud Volumes Service (CVS).  

*NetApp Cloud Sync* service offers a simple and secure way to migrate data to any target, in the cloud or on-premises. Cloud Sync seamlessly transfers and synchronizes data between on-premises or cloud storage, NAS, and object stores.  

*NetApp XCP* is a client software that enables fast and reliable any-to-NetApp and NetApp-to-NetApp data migrations. XCP also provides the capability of moving bulk data efficiently from Hadoop HDFS file systems into ONTAP NFS, S3 or StorageGRID and XCP file analytics provides visibility into the file system. 

*NetApp DataOps Toolkit* is a Python library that makes it simple for data scientists, DevOps, and data engineers to perform various data management tasks, such as near-instantaneously provisioning, cloning, or snapshotting a data volume or JupyterLab workspace that are backed by high-performance, scale-out NetApp storage. It can also rapidly provision new NVIDIA Triton Inference Server instances that are backed by enterprise-class NetApp storage. 

*NetApp’s product security and data centric posture on Zero Trust*

AI developers and security teams must consider the vulnerabilities associated with AI applications leveraging LLMs. As outlined by OWASP1(Open Worldwide Application Security Project), security and safety issues such as data poisoning, data leakage, denial of service and prompt injections within LLMs can impact businesses from data exposure to unauthorized access serving attackers. NetApp understands the importance of data security, making sure customer data is available resisting denial of service attacks. NetApp products are equipped with strict role-based access control (RBAC) measures to control administrative access, as well as secure protocols, audit logging, and industry standard encryption. NetApp offers both software- and hardware-based encryption technologies for securing both data at rest and in transit, and a complete data-centric approach to Zero Trust in which the storage management system protects and monitors access of customer’s data. In particular, the FPolicy™ Zero Trust engine and the FPolicy partner ecosystems identify threats. Furthermore, NetApp ONTAP and SnapLock solutions form a powerful and innovative solution against ransomware. 

*ONTAP AI with DGX BasePOD*

NetApp® ONTAP® AI reference architecture with NVIDIA DGX BasePOD is a scalable architecture for machine learning (ML) and artificial intelligence (AI) workloads. For the critical training phase of LLMs, data is typically copied from the data storage into the training cluster at regular intervals. The servers that are used in this phase use GPUs to parallelize computations, creating a tremendous appetite for data. Meeting the raw I/O bandwidth needs is crucial for maintaining high GPU utilization. 

*ONTAP AI with NVIDIA AI Enterprise*

NVIDIA AI Enterprise is an end-to-end, cloud-native suite of AI and data analytics software that is optimized, certified, and supported by NVIDIA to run on VMware vSphere with NVIDIA-Certified Systems. This software facilitates the simple and rapid deployment, management, and scaling of AI workloads in the modern hybrid cloud environment. NVIDIA AI Enterprise, powered by NetApp and VMware, delivers enterprise-class AI workload and data management in a simplified, familiar package.  

*ONTAP supports GPU Direct Storage*

NetApp® ONTAP® supports NVIDIA GPU Direct Storage™ with the use of NFS over RDMA. NetApp customers can get more than 171GiBps from an ONTAP storage cluster to a single NVIDIA DGX compute node2. Data scientists can achieve the highest levels of performance for machine learning including generative AI workloads, using data center–standard protocols and technologies to deliver the simplest deployment and operational experience. 

*NetApp BlueXP*

NetApp BlueXP™ combines storage and data services via its unified control plane, enabling customers through one single SaaS-delivered point of control to how hybrid, multicloud environments are managed, optimized, and controlled. It enables operational simplicity through the power of AIOps, with the flexible consumption parameters and integrated protection required for today’s cloud-led AI deployments. The benefits of the cloud can be leveraged in several ways. Customers can use GPU instances for computation, and cloud for cold storage tiering and for archives and backups. With most AI applications, the data spans across the edge and/or the core and/or the cloud, which demands AI leaders the ability to orchestrate data across these environments.  

== NetApp Partner Solution Suite  
In addition to its core data capabilities, NetApp has been building a robust network of AI partners like NVIDIA and Domino Data Labs to broaden the AI solution offerings. 

*NVIDIA Guardrails*

Guardrails in AI systems serve as safeguards to ensure the ethical and responsible use of AI technologies. AI developers can choose to define the behavior of LLM-powered applications on specific topics and prevent them from engaging in discussions on unwanted topics. Guardrails, an open-source toolkit, provides the ability to connect an LLM to other services, seamlessly and securely for building trustworthy, safe, and secure LLM conversational systems. 

*Domino Data Lab*

Domino supports the end-to-end data science lifecycle from ideation to production. The Domino enterprise MLOps platform provides an integrated Model Factory that lets customers develop, deploy, and monitor large models in one place using preferred tools and languages. Domino enterprise MLOps platform offers a self-service infrastructure portal for one-click, governed access to the data, tools, and compute the AI team needs for LLMOps. 

*Modzy MLOps and Edge AI*  

NetApp® and Modzy have partnered together to deliver a new way of applying AI at scale to any type of data, including imagery, audio, text, and tables. Modzy is an MLOps platform for deploying, integrating, and running AI models, offers data scientists the capabilities of model monitoring, drift detection and explainability, with an integrated solution to automate the data labeling and model retraining process, ensuring models are retrained with real prediction data to improve model performance over time. Modzy turns AI models into API endpoints that can be integrated anywhere, with governance to manage the full lifecycle of the models.  

== End to end customer scenario – NetApp data services 
NetApp’s data mobility, data governance and data security technologies across the edge, private data center, hybrid and multicloud ecosystem play central role to the data pipeline of generative AI models, datasets, and its various stages of both traditional ML and modern LLM operations. NetApp provides strategic data management capabilities such as SnapMirror® to replicate volume snapshots and optimally transfer data at the edge to on-premises data center or to the cloud. NetApp® DataOps Toolkit enables data scientists perform various data management tasks, such as near-instantaneously provisioning, cloning, or snapshotting a data volume or JupyterLab workspace while A/B testing and validating generative AI applications for downstream NLP tasks. NetApp minimizes the design challenges by accelerating the AI workflow for data scientists working with multiple copies of real-world data or synthetic datasets for deploying context-aware NLP inferencing systems. NetApp® ONTAP AI® for model training and ONTAP® support for NVIDIA GPU Direct Storage offer a well-balanced performance to read and process source datasets from storage into memory numerous times to foster intelligence where model biases and data governance issues can inhibit organizations from scaling access to LLMs. NetApp understands the importance of data security, making sure customer data is available, resisting denial of service attacks. NetApp also partners with a network of AI leaders to deliver customers a comprehensive data pipeline equipped with guardrails and LLMOps, for data preparation to model training, fine-tuning, inferencing, and monitoring of LLM based generative AI applications. 

== Conclusion  

While generative AI like LLM has achieved remarkable milestones, it is crucial to recognize its limitations, design challenges and risks. LLMs rely on large training datasets, which could introduce biases present in the data. These risks can correspond to constraints for LLMs from potential data management challenges associated with data quality, data security, and data mobility. Biased training data can lead to inaccurate outcomes generated by the models that can put both businesses and consumers in jeopardy. NetApp helps organizations meet the complexities created by rapid data growth, multi-cloud management, and the adoption of generative AI. Infrastructure for AI at scale and efficient data management plays a vital role in defining the success of AI applications like generative AI. It is critical to take a holistic approach to cover all deployment scenarios without compromising on the ability to expand as enterprises need to, while keeping costs and ethical AI in check along the data pipeline. NetApp is working with a strong network of AI partners to help customers simplify and accelerate generative AI deployments. 



