---
sidebar: sidebar
permalink: ai/aipod_nv_storage.html
keywords: NetApp AI, AI, Artificial Intelligence, ML, Machine Learning, NVIDIA, NVIDIA AI Enterprise, NVIDIA BasePOD, NVIDIA DGX
summary: NetApp AI Pod with NVIDIA DGX Systems - Storage System Design and Sizing Guidance
---

= NetApp AI Pod with NVIDIA DGX Systems - Storage System Design and Sizing Guidance
:hardbreaks:
:nofooter:
:icons: font
:linkattrs:
:imagesdir: ./../media/

link:aipod_nv_architecture.html[Previous: NetApp AI Pod with NVIDIA DGX Systems - Architecture.]

== Storage system design
Each AFF A800 storage system is connected using four 100 GbE ports from each controller. Two ports from each controller are used for workload data access from the DGX systems, and two ports from each controller are configured as an LACP interface group to support access from the management plane servers for cluster management artifacts and user home directories. All data access from the storage system is provided through NFS, with a storage virtual machine (SVM) dedicated to AI workload access and a separate SVM dedicated to cluster management uses. 

The workload SVM is configured with a total of four logical interfaces (LIFs), with two LIFs on each storage VLAN. Each physical port hosts a two LIFs, resulting in two LIFs per VLAN on each controller. This configuration provides maximum bandwidth as well as the means for each LIF to fail over to another port on the same controller, so that both controllers stay active in the event of a network failure. This configuration also support NFS over RDMA to enable GPUDirect Storage access. Storage capacity is provisioned in the form of a single large FlexGroup volume that spans both controllers. This FlexGroup is accessible from any of the LIFs on the SVM, and mount points from the DGX A100 systems are distributed across the available LIFs for load balancing. 

The management SVM only requires a single LIF, which is hosted on the 2-port interface groups configured on each controller. Other FlexGroup volumes are provisioned on the management SVM to house cluster management artifacts like cluster node images, system monitoring historical data, and end-user home directories. The drawing below shows the logical configuration of the storage system.

image:oai_basepod1_logical.png[Error: Missing Graphic Image]

== Storage System Sizing Guidance

This architecture is intended as a reference for customers and partners who would like to implement a DL infrastructure with NVIDIA DGX systems and NetApp AFF storage systems. The table below shows a rough estimate of the number of A100 and H100 GPUs supported on each AFF model.

image:oai_sizing.png[Error: Missing Graphic Image]

As demonstrated in link:https://www.netapp.com/pdf.html?item=/media/21793-nva-1153-design.pdf[previous versions of this reference architecture], the AFF A800 system easily supports the DL training workload generated by eight DGX A100 systems. Estimates for other storage systems above were calculated based on these results, and estimates for H100 GPUs were calculated by doubling the storage throughput required for A100 systems.  For larger deployments with higher storage performance requirements, additional AFF systems can be added to the NetApp ONTAP cluster up to 12 HA pairs (24 nodes) in a single cluster. Using the FlexGroup technology described in this solution, a 24-node cluster can provide over 40 PB and up to 300 GBps throughput in a single namespace. Other NetApp storage systems such as the AFF A400, A250 and C800 offer lower performance and/or higher capacity options for smaller deployments at lower cost points. Because ONTAP 9 supports mixed-model clusters, customers can start with a smaller initial footprint and add more or larger storage systems to the cluster as capacity and performance requirements grow. 
link:aipod_nv_conclusion.html[Next: NetApp AI Pod with NVIDIA DGX Systems - Conclusion.]