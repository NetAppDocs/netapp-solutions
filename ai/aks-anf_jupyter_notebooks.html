---
sidebar: sidebar
permalink: ai/aks-anf_jupyter_notebooks.html
keywords: introduction, tr-4904, 4904, tr4904, kubernetes, azure, anf, rapids, dask, ml, ai, machine learning, artificial intelligence,
summary: This solution follows the lifecycle of an AI/ML application. We start with the work of data scientists to define the different steps needed to prepare data and train models. By leveraging RAPIDS on Dask, we perform distributed training across the Azure Kubernetes Service (AKS) cluster to drastically reduce the training time when compared to the conventional Python scikit-learn approach. To complete the full cycle, we integrate the pipeline with Azure NetApp Files.
---

= Jupyter notebooks for reference
:hardbreaks:
:nofooter:
:icons: font
:linkattrs:
:imagesdir: ./../media/

link:aks-anf_dataset_and_model_versioning_using_netapp_dataops_toolkit.html[Previous: Dataset and Model Versioning using NetApp DataOps Toolkit.]

There are two Jupyter Notebooks associated with this Technical Report. They are the following:

* image:CTR-PandasRF-collated.ipynb This notebook loads Day 15 from the Criteo Terabyte Click Logs dataset, processes and formats data into a Pandas DataFrame, trains a Scikit-learn random forest model, performs prediction, and calculates accuracy.
* image:criteo_dask_RF.ipynb This notebook loads Day 15 from the Criteo Terabyte Click Logs dataset, processes and formats data into a Dask cuDF, trains a Dask cuML random forest model, performs prediction, and calculates accuracy. By leveraging multiple worker nodes with GPUs, this distributed data and model processing and training approach is highly efficient. The more data you process, the greater the time savings versus a conventional ML approach. You can deploy this notebook in the cloud, on-premises, or in a hybrid environment where your Kubernetes cluster contains compute and storage in different locations, as long as your networking setup enables the free movement of data and model distribution.

link:aks-anf_conclusion.html[Next: Conclusion.]
