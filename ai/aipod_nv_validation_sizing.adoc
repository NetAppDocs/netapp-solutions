---
sidebar: sidebar
permalink: ai/aipod_nv_conclusion.html
keywords: NetApp AI, AI, Artificial Intelligence, ML, Machine Learning, NVIDIA, NVIDIA AI Enterprise, NVIDIA BasePOD, NVIDIA DGX
summary: NetApp AIPod with NVIDIA DGX Systems - Conclusion
---

= NetApp AIPod with NVIDIA DGX Systems - Conclusion
:hardbreaks:
:nofooter:
:icons: font
:linkattrs:
:imagesdir: ./../media/

link:aipod_nv_storage.html[Previous: NetApp AIPod with NVIDIA DGX Systems - Solution Configuration Details]

== Solution Validation

This storage configuration in this solution was validated using a series of synthetic workloads using the open-source tool FIO. These tests include read and write I/O patterns intended to simulate the storage workload generated by DGX systems performing deep learning training jobs. The storage configuration was validated using a cluster of 2-socket CPU servers running the FIO workloads concurrently to simulate a cluster of DGX systems. Each client was configured with the same network configuration described previously, with the addition of the following details.

The following mount options were used for this validation-
• vers=4.1                  # enables pNFS for parallel access to multiple storage nodes
• proto=rdma                # sets the transfer protocol to RDMA instead of the default TCP
• port=20049                # specify the correct port for the RDMA NFS service
• max_connect=16            # enables NFS session trunking to aggregate storage port bandwidth
• write=eager               # improves write performance of buffered writes
• rsize=262144,wsize=262144 # sets the I/O transfer size to 256k

In addition clients were configured with an NFS max_session_slots value of 1024.

The storage system was configured as described with two A900 HA pairs (4 controllers) with two NS224 disk shelves of 24 1.9TB NVMe disk drives. As noted in the architecture section, storage capacity from all controllers was combined using a FlexGroup volume, and data from all clients was distributed across all the controllers in the cluster. This configuration delivered over 50GB/s read throughput, and over 20GB/s sustained write throughput. 

== Storage System Sizing Guidance

This architecture is intended as a reference for customers and partners who would like to implement a DL infrastructure with NVIDIA DGX systems and NetApp AFF storage systems. The table below shows a rough estimate of the number of A100 and H100 GPUs supported on each AFF model.

image:aipod_nv_sizing.png[Error: Missing Graphic Image]

As demonstrated in the validation testing above, the solution as tested can easily support a cluster of eight DGX H100 systems. For larger deployments with higher storage performance requirements, additional AFF systems can be added to the NetApp ONTAP cluster up to 12 HA pairs (24 nodes) in a single cluster. Using the FlexGroup technology described in this solution, a 24-node cluster can provide over 40 PB and up to 300 GBps throughput in a single namespace. Other NetApp storage systems such as the AFF A400, A250 and C800 offer lower performance and/or higher capacity options for smaller deployments at lower cost points. Because ONTAP 9 supports mixed-model clusters, customers can start with a smaller initial footprint and add more or larger storage systems to the cluster as capacity and performance requirements grow.

link:aipod_nv_additional_information.html[Next: NetApp AIPod with NVIDIA DGX Systems - Conclusion and Additional Information]