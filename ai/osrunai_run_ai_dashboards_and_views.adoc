---
sidebar: sidebar
permalink: ai/osrunai_run_ai_dashboards_and_views.html
keywords:
summary:
---

= Run:AI Dashboards and Views
:hardbreaks:
:nofooter:
:icons: font
:linkattrs:
:imagesdir: ./../media/

//
// This file was created with NDAC Version 2.0 (August 17, 2020)
//
// 2020-09-11 12:14:20.431751
//

[.lead]
After installing Run:AI on your Kubernetes cluster and configuring the containers correctly, you see the following dashboards and views on https://app.run.ai/[https://app.run.ai^] in your browser, as shown in the following figure.

image:osrunai_image3.png[Error: Missing Graphic Image]

There are 16 total GPUs in the cluster provided by two DGX-1 nodes. You can see the number of nodes, the total available GPUs, the allocated GPUs that are assigned with workloads, the total number of running jobs, pending jobs, and idle allocated GPUs. On the right side, the bar diagram shows GPUs per Project, which summarizes how different teams are using the cluster resource. In the middle is the list of currently running jobs with job details, including job name, project, user, job type, the node each job is running on, the number of GPU(s) allocated for that job, the current run time of the job, job progress in percentage, and the GPU utilization for that job. Note that the cluster is under-utilized (GPU utilization at 23%) because there are only three running jobs submitted by a single team (`team-a`).

In the following section, we show how to create multiple teams in the Projects tab and allocate GPUs for each team to maximize cluster usage and manage resources when there are many users per cluster. The test scenarios mimic enterprise environments in which memory and GPU resources are shared among training, inferencing, and interactive workloads.

link:osrunai_creating_projects_for_data_science_teams_and_allocating_gpus.html[Next: Creating Projects for Data Science Teams and Allocating GPUs]
