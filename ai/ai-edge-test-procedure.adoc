---
sidebar: sidebar
permalink: ai/ai-edge-test-procedure.html
keywords: procedure, operating system, ubuntu, nvidia, docker, criteo, brats
summary: This section describes the test procedures used to validate this solution.
---
= Test procedure
:hardbreaks:
:nofooter:
:icons: font
:linkattrs:
:imagesdir: ./../media/

//
// This file was created with NDAC Version 2.0 (August 17, 2020)
//
// 2021-10-18 12:10:22.528116
//

[.lead]
This section describes the test procedures used to validate this solution.

== Operating system and AI inference setup

For AFF C190, we used Ubuntu 18.04 with NVIDIA drivers and docker with support for NVIDIA GPUs and used MLPerf https://github.com/mlperf/inference_results_v0.7/tree/master/closed/Lenovo[code^] available as a part of the Lenovo submission to MLPerf Inference v0.7.

For EF280, we used Ubuntu 20.04 with NVIDIA drivers and docker with support for NVIDIA GPUs and MLPerf https://github.com/mlcommons/inference_results_v1.1/tree/main/closed/Lenovo[code^] available as a part of the Lenovo submission to MLPerf Inference v1.1.

To set up the AI inference, follow these steps:

. Download datasets that require registration, the ImageNet 2012 Validation set, Criteo Terabyte dataset, and BraTS 2019 Training set, and then unzip the files.
. Create a working directory with at least 1TB and define environmental variable `MLPERF_SCRATCH_PATH` referring to the directory.
+
You should share this directory on the shared storage for the network storage use case, or the local disk when testing with local data.

. Run the make `prebuild` command, which builds and launches the docker container for the required inference tasks.
+
[NOTE]
The following commands are all executed from within the running docker container:

** Download pretrained AI models for MLPerf Inference tasks: `make download_model`
** Download additional datasets that are freely downloadable: `make download_data`
** Preprocess the data: make `preprocess_data`
** Run: `make build`.
** Build inference engines optimized for the GPU in compute servers: `make generate_engines`
** To run Inference workloads, run the following (one command):

....
make run_harness RUN_ARGS="--benchmarks=<BENCHMARKS> --scenarios=<SCENARIOS>"
....

== AI inference runs

Three types of runs were executed:

* Single server AI inference using local storage
* Single server AI inference using network storage
* Multi-server AI inference using network storage
