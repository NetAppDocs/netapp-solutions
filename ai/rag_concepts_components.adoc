---
sidebar: sidebar
permalink: ai/rag_concepts_components.html
keywords: RAG, Retrieval Augmented Generation, NetApp AI, AI, Artificial Intelligence, ML, Machine Learning, NVIDIA, NeMo, NIM, NIMS, Hybrid, Hybrid Cloud, Hybrid Multicloud, NetApp ONTAP, FlexCache, SnapMirror, BlueXP
summary: Enterprise RAG with NetApp - Concepts and Components
---
= Concepts and Components
:hardbreaks:
:nofooter:
:icons: font
:linkattrs:
:imagesdir: ./../media/


[.lead]

== Generative AI
AI systems such as generative AI are designed by applying unsupervised or self-supervised machine learning to a large dataset. Unlike traditional machine learning models that make predictions about a specific dataset, generative AI models are capable of generating new content like text, code, image, video or audio, in response to user prompts. For this reason, the capabilities of a generative AI system is also classified based on the modality or type of the data used. These could be either unimodal or multimodal. A unimodal system takes only one type of input (Eg. text-only or image-only), whereas a multimodal system can take more than one type of input (Eg. text, image and audio), simultaneously understand and generate content across different modalities. In essence, generative AI is changing how enterprises create content, generate new design concepts, and extract value from existing data.

=== Large Language Models (LLMs)
LLMs are deep learning models pre-trained on vast amounts of data, which can recognize and generate text, among other tasks. LLMs started as a subset of generative AI that primarily focused on language, however, such distinctions are slowly fading as multimodal LLMs continue to emerge. The underlying transformer in an LLM, introduces a novel network architecture other than RNN or CNN. It has a set of neural networks that consist of an encoder and a decoder which helps extract meaning from a sequence of text and understand the relationship between words. LLMs can respond to natural human language and use data analysis to answer an unstructured question. However, LLMs can only be as reliable as the data they ingest, thus prone to hallucinations, stemming from the challenges of garbage in–garbage out. If LLMs are fed with false information, they can generate inaccurate outputs in response to user queries simply to fit the narrative it’s building. Our evidence based research suggests that AI engineers rely on various methods to counteract these hallucinations, one through guardrails that limit inaccurate outcomes and the other by fine-tuning and transfer learning with quality data that is contextually relevant, through techniques like RAG.

=== Retrieval Augmented Generation (RAG)
LLMs are trained on large volumes of data but they aren't trained on your data. RAG solves this problem by adding your data to the data LLMs already have access to. RAG allows customers to leverage the power of a LLM, trained on their data, thus retrieving information and using that to provide contextual information for generative AI users. RAG is a  machine learning technique, an architectural approach that can help reduce hallucinations and improve the efficacy and reliability of LLMs, accelerate the development of AI applications and augment the enterprise search experience.

=== Ragas
There are existing tools and frameworks that help you build RAG pipelines, but evaluating it and quantifying your pipeline performance can be hard. This is where Ragas (RAG Assessment) comes in. Ragas is a framework that helps you evaluate your RAG pipelines. Ragas aims to create an open standard, providing developers with the tools and techniques to leverage continual learning in their RAG applications. For more information, refer to https://docs.ragas.io/en/stable/getstarted/index.html[Get Started with Ragas^]

== Llama 3
Meta's Llama 3, a decoder-only transformer model, is an openly accessible pre-trained large language model (LLM). Trained on over 15 Trillion token of data, Llama 3 is a game changer in natural language understanding (NLU). It excels at contextual understanding and complex tasks like translation and dialogue generation. Llama 3 comes in two sizes: 8B for efficient deployment and development, and 70B for large-scale AI native applications. Customers may deploy Llama 3 on Google Cloud through Vertex AI, on Azure via Azure AI Studio and on AWS through Amazon Sagemaker.

In our validation we have deployed Meta's Llama model with NVIDIA NeMo™ microservices, in an NVIDIA DGX instance compute accelerated with NVIDIA A100 GPUs, to customize and evaluate a generative AI use case, while supporting retrieval-augmented generation (RAG) in applications on-premise.

== Open Source Frameworks
The following additional information about open-source technologies might be relevant depending on your deployment. 

=== LangChain
LangChain is an open-source integration framework for developing applications powered by large language models (LLMs). Customers can efficiently build RAG applications as it comes with Document Loader, VectorStores and various other packages, which enables developers the flexibility to build complex workflows. They can also inspect, monitor, and evaluate apps with LangSmith to constantly optimize and deploy any chain into a REST API with LangServe. LangChain encodes best practices for RAG applications and provides standard interfaces for various components needed to build RAG applications.

=== LlamaIndex
LlamaIndex is a simple, flexible data framework for connecting custom data sources to large language model (LLM) based applications. It lets you ingest data from APIs, databases, PDFs, and more via flexible data connectors. LLMs like Llama 3 and GPT-4 come pre-trained on massive public datasets, allowing for incredible natural language processing capabilities out of the box. However, their utility is limited without access to your own private data. LlamaIndex provides hugely popular Python and TypeScript libraries and is leading the industry in retrieval-augmented generation (RAG) techniques.

== NVIDIA NeMo Microservices
NVIDIA NeMo is an end-to-end platform for building and customizing enterprise-grade generative AI models that can be deployed anywhere, across cloud and data centers. NeMo provides microservices that simplify the generative AI development and deployment process at scale, allowing organizations to connect LLMs to their enterprise data sources. As of the time of this writing, the NeMo Microservices are available via an early access program from NVIDIA.

=== NVIDIA NeMo Inference Microservices (NIMS)
NVIDIA NIMS, part of NVIDIA AI Enterprise, provides a streamlined path for developing AI-powered enterprise applications and deploying AI models in production. NIMS is a containerized inference microservice including industry-standard APIs, domain-specific code, optimized inference engines, and enterprise runtime.

=== NVIDIA NeMo Retriever
NVIDIA NeMo Retriever, the latest service in the NVIDIA NeMo framework, optimizes the embedding and retrieval part of RAG to deliver higher accuracy and more efficient responses. NVIDIA NeMo Retriever is an information retrieval service that can be deployed on-premises or in the cloud. It provides a secure and simplified path for enterprises to integrate enterprise-grade RAG capabilities into their customized production AI applications.

== NVIDIA Enterprise RAG LLM Operator
The NVIDIA Enterprise Retrieval Augmented Generation (RAG) Large Language Model (LLM) Operator enables software components and services that are necessary to run RAG pipelines in Kubernetes. It provides early access to an Operator that manages the life cycle of the key components for RAG pipelines, such as NVIDIA Inference Microservice and NVIDIA NeMo Retriever Embedding Microservice. For more information, refer to https://docs.nvidia.com/ai-enterprise/rag-llm-operator/0.4.1/index.html[NVIDIA Enterprise RAG LLM Operator^]

== Vector Database

=== PostgreSQL: pgvector
With its native bindings for many classical ML algorithms such as XGBoost, doing machine learning with SQL is not something new to PostgreSQL. Lately, with its release of pgvector, an open-source extension for vector similarity search, PostgreSQL has  the capability of storing and searching ML-generated embeddings, a feature useful for AI use cases and applications that use LLMs. 

The default sample pipeline in our validation with NVIDIA Enterprise RAG LLM operator, starts the pgvector database in a pod. The query server then connects to the pgvector database to store and retrieve the embeddings. The chat bot web application and query server communicates with the microservices and the vector database to respond to user prompts.

=== Milvus
As a versatile vector database that offers an API, much like MongoDB, Milvus stands out due to its support for a wide variety of data types, and features like multi-vectorization, thus making it a popular choice for data science and machine learning. It has the capacity to store, index, and manage over a billion embedding vectors generated by Deep Neural Networks (DNN) and Machine Learning (ML) models. Customers can build a RAG application using Nvidia NIM & NeMo microservice and Milvus as vector database. Once NVIDIA NeMo container is sucessfully deployed for embedding generation, Milvus container can be deployed for storing those embeddings. For more information about vector databases and NetApp, see https://docs.netapp.com/us-en/netapp-solutions/ai/vector-database-solution-with-netapp.html[Reference Architecture – Vector Database solution with NetApp^].

=== Apache Cassandra
Apache Cassandra®, an open source NoSQL, highly scalable and highly available database. It ships with vector search capabilities and supports vector data types and vector similarity search functions, particularly useful for AI applications which involve LLMs and private RAG pipelines. 

NetApp Instaclustr provides a fully managed service for Apache Cassandra®, hosted either in the cloud or on-premises. It enables NetApp customers to provision an Apache Cassandra® cluster and connect to the cluster using C#, Node.js, AWS PrivateLink and various other options through the Instaclustr Console or the Instaclstr provisioning API. 

Furthermore, NetApp ONTAP serves as a persistent storage provider for containerized Apache Cassandra cluster running on Kubernetes. NetApp Astra Control seamlessly extends the data management benefits of ONTAP to data-rich Kubernetes applications such as Apache Cassandra. For more information on that, refer to https://cloud.netapp.com/hubfs/SB-4134-0321-DataStax-Cassandra-Guide%20(1).pdf[Application-aware data management for DataStax Enterprise with NetApp Astra Control and ONTAP storage^]

=== NetApp Instaclustr
Instaclustr helps organizations deliver applications at scale by supporting their data infrastructure through its SaaS platform for open source technologies. Generative AI developers who want to embed semantic understanding into their search applications have a multitude of options. Instaclustr for Postgres supports pgvector extensions. Instaclustr for OpenSearch supports vector search to retrieve relevant documents based on input queries along with nearest neighbor functions. Instaclustr for Redis can store vector data, retrieve vectors, and perform vector searches. For more information, read https://www.instaclustr.com/platform/[The Instaclustr Platform by NetApp^]

== NetApp BlueXP

NetApp BlueXP unifies all of NetApp’s storage and data services into a single tool that lets you build, protect, and govern your hybrid multicloud data estate. It delivers a unified experience for storage and data services across on-premises and cloud environments, and enables operational simplicity through the power of AIOps, with the flexible consumption parameters and integrated protection required for today’s cloud-led world.

== NetApp Cloud Insights
NetApp Cloud Insights is a cloud infrastructure monitoring tool that gives you visibility into your complete infrastructure. With Cloud Insights, you can monitor, troubleshoot and optimize all your resources including your public clouds and your private data centers. Cloud Insights delivers full-stack visibility of infrastructure and applications from hundreds of collectors for heterogeneous infrastructure and workloads,  including Kubernetes, all in one place. For more information, refer to https://docs.netapp.com/us-en/cloudinsights/index.html[What can Cloud Insights do for me?^]

== NetApp StorageGRID
NetApp StorageGRID is a software-defined object storage suite that supports a wide range of use cases across public, private, and hybrid multicloud environments. StorageGRID offers native support for the Amazon S3 API and delivers industry-leading innovations such as automated lifecycle management to store, secure, protect, and preserve unstructured data cost effectively over long periods.

== NetApp Spot
Spot by NetApp automates and optimizes your cloud infrastructure in AWS, Azure or Google Cloud to deliver SLA-backed availability and performance at the lowest possible cost. Spot uses machine learning and analytics algorithms that enable you to utilize spot capacity for production and mission-critical workloads. Customers running GPU based instances can benefit from Spot and lower their compute cost.

== NetApp ONTAP

ONTAP 9, the latest generation of storage management software from NetApp, enables businesses to modernize infrastructure and transition to a cloud-ready data center. Leveraging industry-leading data management capabilities, ONTAP enables the management and protection of data with a single set of tools, regardless of where that data resides. You can also move data freely to wherever it is needed: the edge, the core, or the cloud. ONTAP 9 includes numerous features that simplify data management, accelerate, and protect critical data, and enable next generation infrastructure capabilities across hybrid cloud architectures.

=== Simplify data management

Data management is crucial to enterprise IT operations and data scientists so that appropriate resources are used for AI applications and training AI/ML datasets. The following additional information about NetApp technologies is out of scope for this validation but might be relevant depending on your deployment.

ONTAP data management software includes the following features to streamline and simplify operations and reduce your total cost of operation:

* Inline data compaction and expanded deduplication. Data compaction reduces wasted space inside storage blocks, and deduplication significantly increases effective capacity. This applies to data stored locally and data tiered to the cloud.
* Minimum, maximum, and adaptive quality of service (AQoS). Granular quality of service (QoS) controls help maintain performance levels for critical applications in highly shared environments.
* NetApp FabricPool. Provides automatic tiering of cold data to public and private cloud storage options, including Amazon Web Services (AWS), Azure, and NetApp StorageGRID storage solution. For more information about FabricPool, see https://www.netapp.com/pdf.html?item=/media/17239-tr4598pdf.pdf[TR-4598: FabricPool best practices^].

=== Accelerate and protect data

ONTAP delivers superior levels of performance and data protection and extends these capabilities in the following ways:

* Performance and lower latency. ONTAP offers the highest possible throughput at the lowest possible latency.
* Data protection. ONTAP provides built-in data protection capabilities with common management across all platforms.
* NetApp Volume Encryption (NVE). ONTAP offers native volume-level encryption with both onboard and External Key Management support.
* Multitenancy and multifactor authentication. ONTAP enables sharing of infrastructure resources with the highest levels of security.

=== Future-proof infrastructure

ONTAP helps meet demanding and constantly changing business needs with the following features:

* Seamless scaling and nondisruptive operations. ONTAP supports the nondisruptive addition of capacity to existing controllers and to scale-out clusters. Customers can upgrade to the latest technologies, such as NVMe and 32Gb FC, without costly data migrations or outages.
* Cloud connection. ONTAP is the most cloud-connected storage management software, with options for software-defined storage and cloud-native instances in all public clouds.
* Integration with emerging applications. ONTAP offers enterprise-grade data services for next generation platforms and applications, such as autonomous vehicles, smart cities, and Industry 4.0, by using the same infrastructure that supports existing enterprise apps.

== Amazon FSx for NetApp ONTAP

Amazon FSx for NetApp ONTAP is a first-party, fully managed AWS service that provides highly reliable, scalable, high-performing, and feature-rich file storage built on NetApp's popular ONTAP file system. FSx for ONTAP combines the familiar features, performance, capabilities, and API operations of NetApp file systems with the agility, scalability, and simplicity of a fully managed AWS service.

== Azure NetApp Files
Azure NetApp Files is an Azure native, first-party, enterprise-class, high-performance file storage service. It supports SMB, NFS, and dual protocols volumes and can be used for use cases such as:

* File sharing.
* Home directories.
* Databases.
* High-performance computing.
* Generative AI.

== Google Cloud NetApp Volumes
Google Cloud NetApp Volumes is a fully managed, cloud-based data storage service that provides advanced data management capabilities and highly scalable performance. NetApp-hosted data can be used in RAG (retrieval-augmented generation) operations for Google’s Vertex AI platform in a previewed toolkit reference architecture.

== NetApp Astra Trident

Astra Trident enables consumption and management of storage resources across all popular NetApp storage platforms, in the public cloud or on premises, including ONTAP (AFF, FAS, Select, Cloud, Amazon FSx for NetApp ONTAP), Element software (NetApp HCI, SolidFire), Azure NetApp Files service, and Cloud Volumes Service on Google Cloud. Astra Trident is a Container Storage Interface (CSI) compliant dynamic storage orchestrator that natively integrates with Kubernetes.

== Kubernetes

Kubernetes is an open source, distributed, container orchestration platform that was originally designed by Google and is now maintained by the Cloud Native Computing Foundation (CNCF). Kubernetes enables the automation of deployment, management, and scaling functions for containerized applications, and is the dominant container orchestration platform in enterprise environments.

