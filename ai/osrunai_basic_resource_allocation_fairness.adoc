---
sidebar: sidebar
permalink: ai/osrunai_basic_resource_allocation_fairness.html
keywords:
summary:
---
= Basic Resource Allocation Fairness
:hardbreaks:
:nofooter:
:icons: font
:linkattrs:
:imagesdir: ./../media/

//
// This file was created with NDAC Version 2.0 (August 17, 2020)
//
// 2020-09-11 12:14:20.756992
//

[.lead]
In this section, we show that, when `team-d` asks for more GPUs (they are under their quota), the system pauses the workloads of `team-b` and `team-c` and moves them into a pending state in a fair-share manner.

For details including job submissions, container images used, and command sequences executed, see the section link:osrunai_testing_details_for_section_4.9.html[Testing Details for Section 4.9].

The following figure shows the resulting cluster utilization, GPUs allocated per team, and pending jobs due to automatic load balancing and preemptive scheduling. We can observe that when the total number of GPUs requested by all team workloads exceeds the total available GPUs in the cluster, Run:AIâ€™s internal fairness algorithm pauses one job each for `team-b` and `team-c` because they have met their project quota. This provides overall high cluster utilization while data science teams still work under resource constraints set by an administrator.

image:osrunai_image9.png[Error: Missing Graphic Image]

The results of this testing scenario demonstrate the following:

* *Automatic load balancing.* The system automatically balances the quota of the GPUs, such that each team is now using their quota. The workloads that were paused belong to teams that were over their quota.
* *Fair share pause.* The system chooses to stop the workload of one team that was over their quota and then stop the workload of the other team. Run:AI has internal fairness algorithms.
