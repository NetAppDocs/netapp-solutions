---
sidebar: sidebar
permalink: ai-edge-test-plan.html
keywords:
summary:
---

= Test plan
:hardbreaks:
:nofooter:
:icons: font
:linkattrs:
:imagesdir: ./media/

//
// This file was created with NDAC Version 2.0 (August 17, 2020)
//
// 2021-10-18 12:10:22.519681
//

[.lead]
This document follows MLPerf Inference v0.7 https://github.com/mlperf/inference_results_v0.7/tree/master/closed/Lenovo[code^], https://github.com/mlperf/inference_results_v0.7/tree/master/closed/Lenovo[ ^]MLPerf Inference v1.1 https://github.com/mlcommons/inference_results_v1.1/tree/main/closed/Lenovo[code^] and https://github.com/mlcommons/inference_policies/blob/master/inference_rules.adoc[rules^]https://github.com/mlcommons/inference_policies/blob/master/inference_rules.adoc[.^] We ran benchmarks designed for inference at the edge as defined in the following tables. 

|===
|Area  |Task  |Model  |Dataset  |QSL size  |Quality  |Multistream latency constraint 

|Vision 
|Image 
classification 
|Resnet50v1.5 
|ImageNet (224x224) 
|1024 
|99% of 
FP32 
|50ms 
|Vision 
|Object detection (large) 
|SSD-
ResNet34 
|COCO 
(1200x1200) 
|64 
|99% of 
FP32 
|66ms 
|Vision 
|Object detection (small) 
|SSD-
MobileNetsv1 
|COCO 
(300x300) 
|256 
|99% of 
FP32 
|50ms 
|Vision 
|Medical image segmentation 
|3D UNET 
|BraTS 2019 
(224x224x160) 
|16 
|99% and 99.9% of 
FP32 
|n/a 
|Speech 
|Speech-to-
text 
|RNNT 
|Librispeech dev-clean 
|2513 
|99% of 
FP32 
|n/a 
|Language 
|Language processing 
|BERT 
|SQuAD v1.1 
|10833 
|99% of 
FP32 
|n/a 
|===

|===
|Area  |Task  |Scenarios 

|Vision 
|Image classification 
|Single stream, offline, multistream 
|Vision 
|Object detection (large) 
|Single stream, offline, multistream 
|Vision 
|Object detection (small) 
|Single stream, offline, multistream 
|Vision 
|Medical image segmentation 
|Single stream, offline 
|Speech 
|Speech-to-text 
|Single stream, offline 
|Language 
|Language processing 
|Single stream, offline 
|===

We performed these benchmarks using the networked storage architecture developed in this validation and compared results to those from local runs on the edge servers previously submitted to MLPerf. The comparison is to determine how much impact the shared storage has on inference performance.
