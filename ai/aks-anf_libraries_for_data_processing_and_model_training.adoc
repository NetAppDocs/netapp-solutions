---
sidebar: sidebar
permalink: ai/aks-anf_libraries_for_data_processing_and_model_training.html
keywords: libraries, cuml, cudf, dask, transform, load, subsetting, transformation, encoding, rapids, estimators
summary: This page lists the libraries and frameworks that were used to build this task. All these components have been fully integrated with Azure’s role-based access and security controls.
---
= Libraries for data processing and model training
:hardbreaks:
:nofooter:
:icons: font
:linkattrs:
:imagesdir: ./../media/

//
// This file was created with NDAC Version 2.0 (August 17, 2020)
//
// 2021-08-12 10:46:35.671861
//

[.lead]
The following table lists the libraries and frameworks that were used to build this task. All these components have been fully integrated with Azure’s role-based access and security controls.

|===
|Libraries/framework |Description

|Dask cuML
|For ML to work on GPU, the https://github.com/rapidsai/cuml/tree/main/python/cuml/dask[cuML library^] provides access to the RAPIDS cuML package with Dask. RAPIDS cuML implements popular ML algorithms, including clustering, dimensionality reduction, and regression approaches, with high-performance GPU-based implementations, offering speed-ups of up to 100x over CPU-based approaches.
|Dask cuDF
|cuDF includes various other functions supporting GPU-accelerated extract, transform, load (ETL), such as data subsetting, transformations, one-hot encoding, and more. The RAPIDS team maintains a https://github.com/rapidsai/cudf/tree/main/python/dask_cudf[dask-cudf library^] that includes helper methods to use Dask and cuDF.
|Scikit Learn
|Scikit-learn provides dozens of built-in machine learning algorithms and models, called estimators. Each https://scikit-learn.org/stable/glossary.html#term-estimators[estimator^] can be fitted to some data using its https://scikit-learn.org/stable/glossary.html#term-fit[fit^] method.
|===

We used two notebooks to construct the ML pipelines for comparison; one is the conventional Pandas scikit-learn approach, and the other is distributed training with RAPIDS and Dask. Each notebook can be tested individually to see the performance in terms of time and scale. We cover each notebook individually to demonstrate the benefits of distributed training using RAPIDS and Dask.
