---
sidebar: sidebar
permalink: ai/rag_nemo_deployment.html
keywords: RAG, Retrieval Augmented Generation, NetApp AI, AI, Artificial Intelligence, ML, Machine Learning, NVIDIA, NeMo, NIM, NIMS, Hybrid, Hybrid Cloud, Hybrid Multicloud, NetApp ONTAP, FlexCache, SnapMirror, BlueXP
summary: Enterprise RAG with NetApp - NeMo Microservices Deployment
---
= NeMo Microservices Deployment
:hardbreaks:
:nofooter:
:icons: font
:linkattrs:
:imagesdir: ./../media/

[.lead]
This section describes the tasks that need to be performed in order to deploy NVIDIA NeMo Microservices alongside NetApp storage. The NVIDIA NeMo Microservices will be deployed using the link:https://docs.nvidia.com/ai-enterprise/rag-llm-operator/0.4.1/index.html[NVIDIA Enterprise RAG LLM Operator].

== Prerequisites

Before you perform the steps that are outlined in this section, we assume that you have already performed the following tasks:

* You already have a working Kubernetes cluster, and you are running a version of Kubernetes that is supported by the NVIDIA Enterprise RAG LLM Operator. For a list of supported Kubernetes versions, refer to the link:https://docs.nvidia.com/ai-enterprise/rag-llm-operator/0.4.1/platform-support.html[RAG LLM Operator documentation.] This Kubernetes cluster can be either on-premises or in the cloud.
* Your Kubernetes cluster includes at least three GPUs that are supported by the NVIDIA Enterprise RAG LLM Operator. For a list of supported GPUs, refer to the link:https://docs.nvidia.com/ai-enterprise/rag-llm-operator/0.4.1/platform-support.html[RAG LLM Operator documentation.]
* You have already installed and configured NetApp Astra Trident in your Kubernetes cluster. For more details on Astra Trident, refer to the link:https://docs.netapp.com/us-en/trident/index.html[Astra Trident documentation]. This solution is compatible with any NetApp physical storage appliance, software-defined instance, or cloud service, that is supported by Trident.

== Use the NVIDIA Enterprise RAG LLM Operator to deploy NVIDIA NeMo Microservices

. If the NVIDIA GPU Operator is not already installed in your Kubernetes cluster, install the NVIDIA GPU Operator by following the instructions outlined in the link:https://docs.nvidia.com/ai-enterprise/rag-llm-operator/0.4.1/install.html#install-the-nvidia-gpu-operator[RAG LLM Operator documentation.]
. Install the NVIDIA Enterprise RAG LLM Operator by following the instructions outlined in the link:https://docs.nvidia.com/ai-enterprise/rag-llm-operator/0.4.1/install.html#install-the-rag-llm-operator[RAG LLM Operator documentation.]
. Create a RAG pipeline using the NVIDIA Enterprise RAG LLM Operator by following the instructions outlined in the link:https://docs.nvidia.com/ai-enterprise/rag-llm-operator/0.4.1/pipelines.html[RAG LLM Operator documentation.]
** When specifying a StorageClass, be sure to specify a StorageClass that utilizes Astra Trident.
** By default, the RAG pipeline will deploy a new pgvector database to serve as the vector store/knowledge base for the RAG deployment. If you wish to use an existing pgvector or Milvus instance instead, follow the instructions outlined in the link:https://docs.nvidia.com/ai-enterprise/rag-llm-operator/0.4.1/vector-database.html[RAG LLM Operator documentation.] For more information on running a vector database with NetApp, refer to the link:https://docs.netapp.com/us-en/netapp-solutions/ai/vector-database-solution-with-netapp.html[NetApp vector database solution documentation.]
