---
sidebar: sidebar
permalink: ai/aipod_nv_deployment.html
keywords: NetApp AI, AI, Artificial Intelligence, ML, Machine Learning, NVIDIA, NVIDIA AI Enterprise, NVIDIA BasePOD, NVIDIA DGX
summary: NetApp AIPod with NVIDIA DGX Systems - Deployment
---

= NetApp AIPod with NVIDIA DGX H100 Systems - Deployment Details
:hardbreaks:
:nofooter:
:icons: font
:linkattrs:
:imagesdir: ./../media/

[.lead]
This section describes the deployment details used during validation of this solution. The IP addresses used are examples and should be modified based on the deployment environment. For more information on specific commands used in the implementation of this configuration please refer to the appropriate product documentation.  

The diagram below show detailed network and connectivity information for DGX H100 systems and AFF A90 controllers. The deployment guidance in the following sections is based on the details in this diagram. 

_NetApp AIpod network configuration_
image:aipod_nv_a90_netdetail.png[Error: Missing Graphic Image]

The following table shows example cabling assignments for up to 16 DGX systems and 2 AFF A90 HA pairs. 

.Cabling example
|===
|Switch & port  |Device |Device port

|switch1 ports 1-16   
|DGX-H100-01 through -16     
|enp170s0f0np0, slot1 port 1

|switch1 ports 17-32  
|DGX-H100-01 through -16     
|enp170s0f1np1, slot1 port 2

|switch1 ports 33-36  
|AFF-A90-01 through -04      
|port e2a

|switch1 ports 37-40  
|AFF-A90-01 through -04      
|port e2b

|switch1 ports 57-64  
|ISL to switch2              
|ports 57-64

|
|
|

|switch2 ports 1-16   
|DGX-H100-01 through -16     
|enp41s0f0np0, slot 2 port 1

|switch2 ports 17-32  
|DGX-H100-01 through -16     
|enp41s0f1np1, slot 2 port 2

|switch2 ports 33-36  
|AFF-A90-01 through -04      
|port e8a

|switch2 ports 37-40  
|AFF-A90-01 through -04      
|port e8b

|switch2 ports 57-64  
|ISL to switch1              
|ports 57-64
|===

The following table shows the software versions for the various components used in this validation.

.Software versions
|===
|Device  |Software version

|NVIDIA SN4600 switches   
|Cumulus Linux v5.9.1     

|NVIDIA DGX system 
|DGX OS v6.2.1 (Ubuntu 22.04 LTS)   

|Mellanox OFED
|24.01

|NetApp AFF A90 
|NetApp ONTAP 9.14.1
|===

== Storage network configuration
This section outlines key details for configuration of the Ethernet storage network. For information on configuring the InfiniBand compute network please see the link:https://nvdam.widen.net/s/nfnjflmzlj/nvidia-dgx-basepod-reference-architecture[NVIDIA BasePOD documentation]. For more details about switch configuration please refer to the link:https://docs.nvidia.com/networking-ethernet-software/cumulus-linux-59/[NVIDIA Cumulus Linux documentation].

The basic steps used to configure the SN4600 switches are outlined below. This process assumes that cabling and basic switch setup (mgmt IP address, licensing, etc) is complete.

. Configure the ISL bond between the switches to enable multi-link aggregation (MLAG) and failover traffic
    * This validation used 8 links to provide more than enough bandwidth for the storage configuration under test 
    * For specific instructions on enabling MLAG please refer to the Cumulus Linux documentation. 
. Configure LACP MLAG for each pair of client ports and storage ports on both switches
    * port swp17 on each switch for DGX-H100-01 (enp170s0f1np1 and enp41s0f1np1), port swp18 for DGX-H100-02, etc (bond1-16)
    * port swp37 on each switch for AFF-A90-01 (e2b and e8b), port swp38 for AFF-A90-02, etc (bond7-20)
    * nv set interface bondX bond member swpX
    * nv set interface bondx bond mlag id X
. Add all ports and MLAG bonds to the default bridge domain
    * nv set int swp1-16,33-36 bridge domain br_default
    * nv set int bond1-20 bridge domain br_default
. Enable RoCE on each switch
    * nv set roce mode lossless
. Configure VLANs- 2 for client ports, 2 for storage ports, 1 for management, 1 for L3 switch to switch 
    * switch 1-
    ** VLAN 3 for L3 switch to switch routing in the event of client NIC failure
    ** VLAN 11 for storage port 1 on each DGX system (enp170s0f0np0, slot1 port 1)
    ** VLAN 21 for port e2a on each AFF A90 storage controller
    ** VLAN 31 for management using the MLAG interfaces to each DGX system and storage controller
    * switch 2-
    ** VLAN 3 for L3 switch to switch routing in the event of client NIC failure
    ** VLAN 12 for storage port 2 on each DGX system (enp41s0f0np0, slot2 port 1)
    ** VLAN 22 for port e8a on each AFF A90 storage controller
    ** VLAN 31 for management using the MLAG interfaces to each DGX system and storage controller
. Assign physical ports to each VLAN as appropriate, e.g. client ports in client VLANs and storage ports in storage VLANs
    * nv set int <swpX> bridge domain br_default access <vlan id>
    * MLAG ports should remain as trunk ports to enable multiple VLANs over the bonded interfaces as needed. 
. Configure switch virtual interfaces (SVI) on each VLAN to act as a gateway & enable L3 routing
    * switch 1-
    ** nv set int vlan3 ip address 192.168.3.0/31
    ** nv set int vlan11 ip address 192.168.11.254/24
    ** nv set int vlan21 ip address 192.168.21.254/24
    * switch 2-
    ** nv set int vlan3 ip address 192.168.3.1/31
    ** nv set int vlan12 ip address 192.168.12.254/24
    ** nv set int vlan22 ip address 192.168.22.254/24
. Create static routes 
    * Static routes are automatically created for subnets on the same switch
    * Additional static routes are required for switch to switch routing in the event of a client link failure
    ** switch 1- 
    *** nv set vrf default router static 192.168.12.0/24 via 192.168.3.1
    *** nv set vrf default router static 192.168.22.0/24 via 192.168.3.1
    ** switch 2- 
    *** nv set vrf default router static 192.168.11.0/24 via 192.168.3.0
    *** nv set vrf default router static 192.168.21.0/24 via 192.168.3.0

== Storage system configuration
This section describes key details for configuration of the A90 storage system for this solution. For more details about configuration of ONTAP systems please refer to the [ONTAP documentation]. The diagram below shows the logical configuration of the storage system. 

_NetApp A90 storage cluster logical configuration_
image:aipod_nv_a90_logical.png[Error: Missing Graphic Image]

The basic steps used to configure the storage system are outlined below. This process assumes that basic storage cluster installation has been completed. 

. Configure 1 aggregate on each controller with all available partitions minus 1 spare
    * aggr create -node <node> -aggregate <node>_data01 -diskcount <47>
. Configure ifgrps on each controller
    * a11a on controller 1, a21a on controller 2, a31a on controller 3, a41a on controller 4
    * net port ifgrp create -node <node> -ifgrp <ifgrp> -mode multimode_lacp -distr-function port
    * net port ifgrp add-port -node <node> -ifgrp <ifgrp> -ports <node>:e2b,<node>:e8b
. Configure mgmt vlan port on ifgrp on each controller
    * net port vlan create -node aff-a90-01 -port a11a -vlan-id 31
    * net port vlan create -node aff-a90-02 -port a21a -vlan-id 31
. Create broadcast domains
    * broadcast-domain create -broadcast-domain vlan21 -mtu 9000 -ports aff-a90-01:e2a,aff-a90-02:e2a,aff-a90-03:e2a,aff-a90-04:e2a
    * broadcast-domain create -broadcast-domain vlan22 -mtu 9000 -ports aff-a90-01:e8a,aff-a90-02:e8a,aff-a90-03:e8a,aff-a90-04:e8a
    * broadcast-domain create -broadcast-domain vlan31 -mtu 9000 -ports aff-a90-01:a11a-31,aff-a90-02:a21a-31,aff-a90-03:a31a-31,aff-a90-04:a41a-31
. Configure management SVM
    * create LIF
    ** net int create -vserver basepod-mgmt -lif vlan31-01 -home-node aff-a90-01 -home-port a11a-31 -address 192.168.31.X -netmask 255.255.255.0
    * create FlexGroup volumes-
    ** vol create -vserver basepod-mgmt -volume home -size 10T -auto-provision-as flexgroup -junction-path /home
    ** vol create -vserver basepod-mgmt -volume cm -size 10T -auto-provision-as flexgroup -junction-path /cm
    * create export policy 
    ** export-policy rule create -vserver basepod-mgmt -policy default -client-match 192.168.31.0/24 -rorule sys -rwrule sys -superuser sys

. Configure data SVM
    * create LIFs
    ** net int create -vserver basepod-data -lif c1-2a-lif1 -home-node aff-a90-01 -home-port e2a -address 192.168.21.101 -netmask 255.255.255.0
    ** net int create -vserver basepod-data -lif c1-2a-lif2 -home-node aff-a90-01 -home-port e2a -address 192.168.21.102 -netmask 255.255.255.0
    ** net int create -vserver basepod-data -lif c1-8a-lif1 -home-node aff-a90-01 -home-port e8a -address 192.168.22.101 -netmask 255.255.255.0
    ** net int create -vserver basepod-data -lif c1-8a-lif2 -home-node aff-a90-01 -home-port e8a -address 192.168.22.102 -netmask 255.255.255.0
    ** net int create -vserver basepod-data -lif c2-2a-lif1 -home-node aff-a90-02 -home-port e2a -address 192.168.21.103 -netmask 255.255.255.0
    ** net int create -vserver basepod-data -lif c2-2a-lif2 -home-node aff-a90-02 -home-port e2a -address 192.168.21.104 -netmask 255.255.255.0
    ** net int create -vserver basepod-data -lif c2-8a-lif1 -home-node aff-a90-02 -home-port e8a -address 192.168.22.103 -netmask 255.255.255.0
    ** net int create -vserver basepod-data -lif c2-8a-lif2 -home-node aff-a90-02 -home-port e8a -address 192.168.22.104 -netmask 255.255.255.0
. Configure LIFs for RDMA access
    * Starting with ONTAP 9.15.1, physical interfaces automatically configure for appropriate DSCP and ToS values for end-to-end RoCE support. Prior to ONTAP 9.15.1, this configuration can be enabled with the help of NetApp Support. 
    * net int modify -vserver 
. Create FlexGroup volumes-
    * vol create -vserver basepod-data -volume data -size 100T -auto-provision-as flexgroup -junction-path /data  
. Create export policy 
    * export-policy rule create -vserver basepod-data -policy default -client-match 192.168.11.0/24 -rorule sys -rwrule sys -superuser sys 
    * export-policy rule create -vserver basepod-data -policy default -client-match 192.168.11.0/24 -rorule sys -rwrule sys -superuser sys
. create routes
    * route add -vserver basepod_data -destination 192.168.11.0/24 -gateway 192.168.21.254 metric 20
    * route add -vserver basepod_data -destination 192.168.11.0/24 -gateway 192.168.22.254 metric 30
    * route add -vserver basepod_data -destination 192.168.12.0/24 -gateway 192.168.22.254 metric 20
    * route add -vserver basepod_data -destination 192.168.12.0/24 -gateway 192.168.21.254 metric 30  

=== DGX H100 configuration for storage access
This section describes key details for configuration of the DGX H100 systems. Many of these configuration items can be included in the OS image deployed to the DGX systems or implemented by Base Command Manager at boot time. They are listed here for reference, for more information on configuring nodes and software images in BCM please see the BCM documentation. 

. Install additional packages
    * ipmitool
    * python3-pip
. Install Python packages
  * paramiko
  * matplotlib
. Reconfigure dpkg after package installation
  * dpkg --configure -a
. Install MOFED
. Set mst values for performance tuning
  * mstconfig -y -d <aa:00.0,29:00.0> set ADVANCED_PCI_SETTINGS=1 NUM_OF_VFS=0 MAX_ACC_OUT_READ=44
. Reset the adapters after modifying settings
  * mlxfwreset -d <aa:00.0,29:00.0> -y reset
. Set MaxReadReq on PCI devices
  * setpci -s <aa:00.0,29:00.0> 68.W=5957
. Set RX and TX ring buffer size
  * ethtool -G <enp170s0f0np0,enp41s0f0np0> rx 8192 tx 8192
. Set PFC and DSCP using mlnx_qos
  * mlnx_qos -i <enp170s0f0np0,enp41s0f0np0> --pfc 0,0,0,1,0,0,0,0 --trust=dscp --cable_len=3
. Set ToS for RoCE traffic on network ports
  * echo 106 > /sys/class/infiniband/<mlx5_7,mlx5_1>/tc/1/traffic_class
. Set NFS max_session_slots
  * echo 1024 > /sys/module/nfs/parameters/max_session_slots
. Set file_max limit
  * echo 999999 > /proc/sys/fs/file-max
. Configure each storage NIC with an IP address on appropriate subnet
. configure static routes for primary & secondary paths to each storage subnet
  * route add –net 192.168.21.0/24 gw 192.168.11.254 metric 20
  * route add –net 192.168.21.0/24 gw 192.168.12.254 metric 30
  * route add –net 192.168.22.0/24 gw 192.168.12.254 metric 20
  * route add –net 192.168.22.0/24 gw 192.168.11.254 metric 30
. Mount /home volume
  * mount -o vers=3,nconnect=16,rsize=262144,wsize=262144 192.168.31.X:/home /home
. Mount /data volume 
  * The following mount options were used when mounting the data volume-
  ** vers=4.1                  # enables pNFS for parallel access to multiple storage nodes
  ** proto=rdma                # sets the transfer protocol to RDMA instead of the default TCP
  ** max_connect=16            # enables NFS session trunking to aggregate storage port bandwidth
  ** write=eager               # improves write performance of buffered writes
  ** rsize=262144,wsize=262144 # sets the I/O transfer size to 256k

