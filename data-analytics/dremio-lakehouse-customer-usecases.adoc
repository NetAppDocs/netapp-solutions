---
sidebar: sidebar
permalink: data-analytics/dremio-lakehouse-customer-usecases.html
keywords: customer use case details
summary: This section covers the customer use case details of Dremio with netapp object storage .
---

= Customer Use Cases
:hardbreaks:
:nofooter:
:icons: font
:linkattrs:
:imagesdir: ../media/

//
// This file was created with NDAC Version 2.0 (August 17, 2020)
//
// 2021-11-15 09:15:45.920602
//

[.lead]
We discuss about two customer use cases

== NetApp ActiveIQ use case
image:activeIQold.png["ActiveIQ old architecture"]
*Challenge*: NetApp's Active IQ solution, initially designed for support use cases, has evolved into a comprehensive offering for both internal users and customers. However, the underlying Hadoop/MapReduce-based backend infrastructure posed challenges due to the rapid growth of data and the need for efficient data access. Scaling storage meant adding unnecessary compute resources, resulting in increased costs. Additionally, managing the Hadoop cluster was time-consuming and required specialized expertise. Data performance and management issues further complicated the situation, with queries taking an average of 45 minutes and potential resource starvation due to misconfigurations. To address these challenges, NetApp sought a solution like Dremio that could reduce costs, decouple storage and compute, improve performance, simplify data management, offer fine-grained controls, and provide disaster recovery capabilities.
*Solution*:
image:activeIQnew.png["ActiveIQ new architecture with dremio"]
Dremio enabled NetApp to modernize its Hadoop-based data infrastructure in a phased approach, providing a roadmap for unified analytics. Unlike other vendors that required significant changes to data processing, Dremio seamlessly integrated with existing pipelines, saving time and expenses during migration. By transitioning to a fully containerized environment, NetApp reduced management overhead, improved security, and enhanced resilience. Dremio's adoption of open ecosystems like Apache Iceberg and Arrow ensured future-proofing, transparency, and extensibility. As a replacement for the Hadoop/Hive infrastructure, Dremio offered functionality for secondary use cases through the semantic layer. While the existing Spark-based ETL and data ingestion mechanisms remained, Dremio provided a unified access layer for easier data discovery and exploration without duplication. This approach significantly reduced data replication factors and decoupled storage and compute.
*Benefits*:
With Dremio, NetApp achieved significant cost reductions by minimizing compute consumption and disk space requirements in their data environments. The new Active IQ Data Lake comprised 8,900 tables holding 3 petabytes of data, compared to the previous infrastructure with over 7 petabytes. The migration to Dremio also involved transitioning from 33 mini-clusters and 4,000 cores to 16 executor nodes on Kubernetes clusters. Despite the decrease in compute resources, NetApp experienced remarkable performance improvements. By directly accessing data through Dremio, query runtime decreased from 45 minutes to 2 minutes, resulting in a 95% faster time to insights for predictive maintenance and optimization. The migration yielded over 60% reduction in compute costs, over 20 times faster queries, and over 30% savings in total cost of ownership (TCO).

==Auto Parts Sales customer use-case.  

*Challenges*: Executive and corporate Financial Planning and Analysis are unable to see consolidated sales reporting and have to read individual line of business sales metrics reports. This results in customer making decisions with data that is 1 day old. The lead time can typically take over 4 weeks. Troubleshooting data pipelines requires additional 3 days to complete. Current performance of reports requires our analyst community to wait for data to process or load, rather than finding insights and driving new business behavior. Today, there are different databases for different lines of businesses. Resulting in numerous data silos. This complicates Data Governance as there are too many ways for analysts to come up with their own version of the truth vs a single source of truth. The current approach is costing $1.9 million in Data platform & people costs. Maintaining the current platform and filling data requests costs roughly 7 Field Technical Engineer(FTE)s per year. With data requests growing, Customer’s data intelligence team will need to scale by 2025.
*Solution*: Cost effectively store and manage large Iceberg tables in Object Store (NetApp). Build Data Domains within Dremio's semantic layer, allowing business users to easily create, search, share data products
*Benefits to customer*: 
•	Improve and optimize existing data architecture to reduce time to insights from 4 weeks to hours
•	Reduce troubleshooting time from 3 days to hours
•	Decrease Data platform & Management costs by over $380,000
•	~2 FTEs of Data Intelligence effort saved per year
