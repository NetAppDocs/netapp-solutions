---
sidebar: sidebar
permalink: databases/azure_ora_nfile_factors.html
summary: This section provides details on factors to be considered when deploy Oracle database on Azure virtual machine and Azure NetApp Files storage.
keywords: database, Oracle, Azure, Azure NetApp Files
---

= Factors to consider for Oracle database deployment
:hardbreaks:
:nofooter:
:icons: font
:linkattrs:
:table-stripes: odd
:imagesdir: ./../media/

link:azure_ora_nfile_architecture.html[Previous: Solution architecture.]

A public cloud provides many choices for compute and storage, and using the correct type of compute instance and storage engine is a good place to start for database deployment. You should also select compute and storage configurations that are optimized for Oracle databases.

The following sections describe the key considerations when deploying Oracle database in Azure public cloud on an Azure virtual machine instance with Azure NetApp Files storage.

== VM type and sizing

Selecting the right VM type and size is important for optimal performance of a relational database in a public cloud. Azure virtual machine provide varieties of compute instances that can be used to host Oracle database workload. Referred to this Microsoft documentation link:https://docs.microsoft.com/en-us/azure/virtual-machines/sizes[Sizes for virtual machines in Azure^] for different type of Azure virtual machines and their sizing. In general, NetApp recommends using a General Purpose Azure virtual machine for small and medium sized Oracle database deployment. For larger Oracle database deployment, a memory optimized Azure virtual machine fits the bill. With bigger available RAM, a larger Oracle SGA or smart flash cache can be configured to reduce the physical IO which in turn improve database performance. Azure NetApp Files works as NFS mount attached to Azure virtual machine, which could offers higher throughput and could outperform storage optimized virtual machine with local storage. Other factors to consider including the following:

* Choose the correct vCPU and RAM combination based on workload characteristics. As the RAM size increases on the VM, so is the number of vCPU cores. There should be a balance at some point as the Oracle license fees are charged on the number of vCPU cores.
* Add swap space to a VM. The default Azure virtual machine deployment does not create a swap space, which is not optimal for a database.

== Azure NetApp Files performance

Azure NetApp Files volumes are allocated from a capacity pool the customer has to provision in his Azure NetApp Files storage account. Each capacity pool is assigned:

* To a service level that defines the overall performance capability.
* The initially provisioned storage capacity or tiering for that capacity pool. A quality of service (QoS) level that defines the overall maximum throughput per provisioned space.

The service level and initially provisioned storage capacity determines the performance level for a particular Oracle database volume.

=== Service Levels for Azure NetApp Files

Azure NetApp Files supports three service levels: Ultra, Premium, and Standard.
• Ultra storage. This tier provides up to 128MiB/s of throughput per 1TiB of volume quota assigned.
• Premium storage. This tier provides up to 64MiB/s of throughput per 1TiB of volume quota assigned.
• Standard storage. This tier provides up to 16MiB/s of throughput per 1TiB of volume quota assigned.

=== Capacity tiering and quality of service

Each of the desired service levels has an associated cost per provisioned capacity and includes a quality of service (QoS) level that defines the overall maximum throughput per provisioned space.

For example, a 10TiB provisioned single capacity pool with premium service level will provide an overall available throughput for all volumes in this capacity pool of 10x 64MBps, so 640MBps, or 40,000 (16K) IOPs or 80,000 (8K) IOPs.

The minimum capacity pool size is 4TiB. You can change the size of a capacity pool in 1-TiB increments in response to changes of your workload requirements.

=== Calculate the service level at a volume level


== Storage layout and settings

NetApp recommends the following storage layout:

* For NFS storage, the recommended volume layout is three volumes: one for the Oracle binary; one for Oracle data and a duplicate control file; and one for the Oracle active log, archived log, and control file.
+
image:aws_ora_fsx_ec2_stor_12.PNG[Error: Missing Graphic Image]

* For iSCSI storage, the recommended volume layout is three volumes: one for the Oracle binary; one for Oracle data and a duplicate control file; and one for the Oracle active log, archived log, and control file. However, each data and log volume ideally should contain four LUNs. The LUNs are ideally balanced on the HA cluster nodes.
+
image:aws_ora_fsx_ec2_stor_13.PNG[Error: Missing Graphic Image]

* For storage IOPS and throughput, you can choose the threshold for provisioned IOPS and throughput for the FSx storage cluster, and these parameters can be adjusted on the fly anytime the workload changes.

** The auto IOPS setting is three IOPS per GiB of allocated storage capacity or user defined storage up to 80,000.

** The throughput level is incremented as follow: 128, 256, 512, 1024, 2045 MBps.

Review the link:https://docs.aws.amazon.com/fsx/latest/ONTAPGuide/performance.html[Amazon FSx for NetApp ONTAP performance^] documentation when sizing throughput and IOPS.

== NFS configuration

Linux, the most common operating system, includes native NFS capabilities. Oracle offers the direct NFS (dNFS) client natively integrated into Oracle. Oracle has supported NFSv3 for over 20 years, and NFSv4 is supported with Oracle 12.1.0.2 and later. Automated Oracle deployment using the NetApp automation toolkit automatically configures dNFS on NFSv3.

Other factors to consider:

* TCP slot tables are the NFS equivalent of host-bus-adapter (HBA) queue depth. These tables control the number of NFS operations that can be outstanding at any one time. The default value is usually 16, which is far too low for optimum performance. The opposite problem occurs on newer Linux kernels, which can automatically increase the TCP slot table limit to a level that saturates the NFS server with requests.
+
For optimum performance and to prevent performance problems, adjust the kernel parameters that control the TCP slot tables to 128.
+
[source, cli]
sysctl -a | grep tcp.*.slot_table

* The following table provides recommended NFS mount options for Linux NFSv3 - single instance.
+
image:aws_ora_fsx_ec2_nfs_01.PNG[Error: Missing Graphic Image]

[NOTE]
Before using dNFS, verify that the patches described in Oracle Doc 1495104.1 are installed. Starting with Oracle 12c, DNFS includes support for NFSv3, NFSv4, and NFSv4.1. NetApp support policies cover v3 and v4 for all clients, but, at the time of writing, NFSv4.1 is not supported for use with Oracle dNFS.

== High availability

As indicated in the solution architecture, HA is built on storage-level replication. Therefore, the startup and availability of Oracle is contingent on how quickly the compute and storage can be brought up and recovered. See the following key factors:

* Have a standby compute instance ready and synced up with the primary through Ansible parallel update to both hosts.

* Replicate the binary volume from the primary for standby purposes so that you do not need to install Oracle at the last minute and figure out what needs to be installed and patched.

* Replication frequency dictates how fast the Oracle database can be recovered to make service available. There is a trade off between the replication frequency and storage consumption.

* Leverage automation to make recovery and switch over to standby quick and free of human error. NetApp provides an automation toolkit for this purpose.

link:azure_ora_nfile_procedures.html[Next: Deployment procedures.]
