---
sidebar: sidebar
permalink: databases/azure_ora_nfile_factors.html
summary: This section provides details on factors to be considered when deploy Oracle database on Azure virtual machine and Azure NetApp Files storage.
keywords: database, Oracle, Azure, Azure NetApp Files
---

= Factors to consider for Oracle database deployment
:hardbreaks:
:nofooter:
:icons: font
:linkattrs:
:table-stripes: odd
:imagesdir: ./../media/

link:azure_ora_nfile_architecture.html[Previous: Solution architecture.]

A public cloud provides many choices for compute and storage, and using the correct type of compute instance and storage engine is a good place to start for database deployment. You should also select compute and storage configurations that are optimized for Oracle databases.

The following sections describe the key considerations when deploying Oracle database in Azure public cloud on an Azure virtual machine instance with Azure NetApp Files storage.

== VM type and sizing

Selecting the right VM type and size is important for optimal performance of a relational database in a public cloud. Azure virtual machine provide varieties of compute instances that can be used to host Oracle database workload. Referred to this Microsoft documentation link:https://docs.microsoft.com/en-us/azure/virtual-machines/sizes[Sizes for virtual machines in Azure^] for different type of Azure virtual machines and their sizing. In general, NetApp recommends using a General Purpose Azure virtual machine for small and medium sized Oracle database deployment. For larger Oracle database deployment, a memory optimized Azure virtual machine fits the bill. With bigger available RAM, a larger Oracle SGA or smart flash cache can be configured to reduce the physical IO which in turn improve database performance.

Azure NetApp Files works as NFS mount attached to Azure virtual machine, which offers higher throughput and overcomes storage optimized virtual machine throughput limit with local storage. Thus running Oracle on Azure NetApp Files could reduce licensable Oracle CPU cores count and license costs. Refer to TR-4780: link:https://www.netapp.com/media/17105-tr4780.pdf[Oracle Databases on Microsoft Azure^], Section 7 - How Does Oracle Licensing Work? 

Other factors to consider including the following:

* Choose the correct vCPU and RAM combination based on workload characteristics. As the RAM size increases on the VM, so is the number of vCPU cores. There should be a balance at some point as the Oracle license fees are charged on the number of vCPU cores.
* Add swap space to a VM. The default Azure virtual machine deployment does not create a swap space, which is not optimal for a database.

== Azure NetApp Files performance

Azure NetApp Files volumes are allocated from a capacity pool the customer has to provision in his Azure NetApp Files storage account. Each capacity pool is assigned:

* To a service level that defines the overall performance capability.
* The initially provisioned storage capacity or tiering for that capacity pool. A quality of service (QoS) level that defines the overall maximum throughput per provisioned space.

The service level and initially provisioned storage capacity determines the performance level for a particular Oracle database volume.

=== 1. Service Levels for Azure NetApp Files

Azure NetApp Files supports three service levels: Ultra, Premium, and Standard.

* Ultra storage. This tier provides up to 128MiB/s of throughput per 1TiB of volume quota assigned.
* Premium storage. This tier provides up to 64MiB/s of throughput per 1TiB of volume quota assigned.
* Standard storage. This tier provides up to 16MiB/s of throughput per 1TiB of volume quota assigned.

=== 2. Capacity tiering and quality of service

Each of the desired service levels has an associated cost per provisioned capacity and includes a quality of service (QoS) level that defines the overall maximum throughput per provisioned space.

For example, a 10TiB provisioned single capacity pool with premium service level will provide an overall available throughput for all volumes in this capacity pool of 10x 64MBps, so 640MBps, or 40,000 (16K) IOPs or 80,000 (8K) IOPs.

The minimum capacity pool size is 4TiB. You can change the size of a capacity pool in 1-TiB increments in response to changes of your workload requirements to manage storage needs and costs.

=== 3. Calculate the service level at a database volume

The throughput limit for an Oracle database volume is determined by the combination of the following factors: The service level of the capacity pool to which the volume belongs
and The quota assigned to the volume.

The following diagram shows how the throughput limit for an Oracle database volume is calculated.

image:db_ora_azure_anf_factors_01.PNG[Error: Missing Graphic Image]

In example 1, a volume from a capacity pool with the Premium storage tier that is assigned 2TiB of quota will be assigned a throughput limit of 128MiB/s (2TiB * 64MiBps). This scenario applies regardless of the capacity pool size or the actual volume consumption.

In example 2, a volume from a capacity pool with the Premium storage tier that is assigned 100GiB of quota will be assigned a throughput limit of 6.25MiBps (0.09765625TiB * 64MiBps). This scenario applies regardless of the capacity pool size or the actual volume consumption.

Please note that the minimum volume size is 100GiB.

== Storage layout and settings

NetApp recommends the following storage layout:

* For small databases, using single volume layout for all Oracle files.
+
image:db_ora_azure_anf_factors_02.PNG[Error: Missing Graphic Image]

* For large databases, the recommended volume layout is multiple volumes: one for Oracle data and a duplicate control file; and one for the Oracle active log, archived log, and control file. It is highly recommended to allocate a volume for the Oracle binary instead of local drive so that database can be relocated to a new host and quickly restored.
+
image:db_ora_azure_anf_factors_03.PNG[Error: Missing Graphic Image]

== NFS configuration

Linux, the most common operating system, includes native NFS capabilities. Oracle offers the direct NFS (dNFS) client natively integrated into Oracle. Oracle dNFS bypasses OS cache and enable parallel processing to improve database performance. Oracle has supported NFSv3 for over 20 years, and NFSv4 is supported with Oracle 12.1.0.2 and later.

By using dNFS (available since Oracle 11g), an Oracle database running on an Azure Virtual Machine can drive significantly more I/O than the native NFS client. Automated Oracle deployment using the NetApp automation toolkit automatically configures dNFS on NFSv3.

Following diagram demonstrates slob benchmark on Azure NetApp Files with Oracle dNFS.

image:db_ora_azure_anf_factors_04.PNG[Error: Missing Graphic Image]

Other factors to consider:

* TCP slot tables are the NFS equivalent of host-bus-adapter (HBA) queue depth. These tables control the number of NFS operations that can be outstanding at any one time. The default value is usually 16, which is far too low for optimum performance. The opposite problem occurs on newer Linux kernels, which can automatically increase the TCP slot table limit to a level that saturates the NFS server with requests.
+
For optimum performance and to prevent performance problems, adjust the kernel parameters that control the TCP slot tables to 128.
+
[source, cli]
sysctl -a | grep tcp.*.slot_table

* The following table provides recommended NFS mount options for Linux NFSv3 - single instance.
+
image:aws_ora_fsx_ec2_nfs_01.PNG[Error: Missing Graphic Image]

[NOTE]
Before using dNFS, verify that the patches described in Oracle Doc 1495104.1 are installed. Starting with Oracle 12c, DNFS includes support for NFSv3, NFSv4, and NFSv4.1. NetApp support policies cover v3 and v4 for all clients, but, at the time of writing, NFSv4.1 is not supported for use with Oracle dNFS.

link:azure_ora_nfile_procedures.html[Next: Deployment procedures.]
