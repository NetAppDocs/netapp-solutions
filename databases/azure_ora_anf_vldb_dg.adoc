---
sidebar: sidebar
permalink: databases/azure_ora_anf_vldb_dg.html
keywords: Oracle, Azure, ANF, Database, Oracle 19c, Data Guard 
summary: "The solution provides an overview and details for configuring high throughput Oracle Very Large Database (VLDB) on Microsoft Azure NetApp Files (ANF) with Oracle Data Guard in Azure cloud."   
---

= TR-5003: High Throughput Oracle VLDB Implementation on ANF 
:hardbreaks:
:nofooter:
:icons: font
:linkattrs:
:imagesdir: ../media/

Allen Cao, Niyaz Mohamed, NetApp

[.lead]
The solution provides an overview and details for configuring a high-throughput Oracle Very Large Database (VLDB) on Microsoft Azure NetApp Files (ANF) with Oracle Data Guard in the Azure cloud.

== Purpose

High throughput and mission-critical Oracle VLDB put a heavy demand on backend database storage. To meet service level agreement (SLA), the database storage must deliver the required capacity and high input/output operations per second (IOPS) while maintaining sub milliseconds latency performance. This is particularly challenging when deploying such a database workload in the public cloud with a shared storage resources environment. Not all storage platforms are created equal. Premium Azure NetApp Files storage in combination with Azure infrastructure can meet the needs of such a highly demanding Oracle workload. In a validated performance benchmark (link:https://learn.microsoft.com/en-us/azure/azure-netapp-files/performance-oracle-multiple-volumes[Oracle database performance on Azure NetApp Files multiple volumes^]), ANF can deliver 2.5 million read IOPS with 700 microseconds latency in a synthetic 100% random select workload via the SLOB tool. With a standard 8k block size, this translates to about 20 GiB/s throughput.

In this documentation, we demonstrate how to set up an Oracle VLDB with Data Guard configuration on ANF storage with multiple NFS volumes and Oracle ASM for storage load balancing. The standby database can be quickly (mins) backed up via snapshot and cloned for read/write access for use cases as desired. NetApp Solutions Engineering team provides an automation toolkit to create and refresh clones with ease.

This solution addresses the following use cases:

* Implementation of Oracle VLDB in a Data Guard setting on Microsoft Azure NetApp Files storage across Azure regions.
* Snapshot backup and clone the physical standby database to serve use cases such as reporting, dev, test, etc. via automation. 

== Audience

This solution is intended for the following people:

* A DBA who sets up Oracle VLDB with Active Data Guard in Azure cloud for high availability, data protection, and disaster recovery.
* A database solution architect interested in Oracle VLDB with Active Data Guard configuration in the Azure cloud.
* A storage administrator who manages Azure NetApp Files storage that supports Oracle database.
* An application owner who likes to stand up Oracle VLDB with Data Guard in an Azure cloud environment.

== Solution test and validation environment

The testing and validation of this solution was performed in an Azure cloud lab setting that might not match the actual user deployment environment. For more information, see the section <<Key factors for deployment consideration>>.

=== Architecture

image:azure_ora_anf_vldb_dg_architecture.png["This image provides a detailed picture of the Oracle Data Guard implementation in Azure cloud on ANF."]

=== Hardware and software components

[width=100%,cols="33%, 33%, 33%", frame=none, grid=rows]
|===
3+^| *Hardware*
| Azure NetApp Files | Current version offered by Microsoft | Two 4 TiB Capacity Pools, Premium Service Level, Auto QoS 
| Azure VMs for DB Servers | Standard B4ms (4 vcpus, 16 GiB memory) | Three DB VMs, one as the primary DB server, one as the standby DB server, and the third as a clone DB server 

3+^| *Software*
| RedHat Linux | Red Hat Enterprise Linux 8.6 (LVM) - x64 Gen2 | Deployed RedHat subscription for testing
| Oracle Grid Infrastructure | Version 19.18 | Applied RU patch p34762026_190000_Linux-x86-64.zip
| Oracle Database | Version 19.18 | Applied RU patch p34765931_190000_Linux-x86-64.zip
| dNFS OneOff Patch | p32931941_190000_Linux-x86-64.zip | Applied to both grid and database
| Oracle OPatch | Version 12.2.0.1.36 | Latest patch p6880880_190000_Linux-x86-64.zip
| SnapCenter | Version 6.0.1 | Build 6.0.1.4487
| NFS | Version 3.0 | dNFS enabled for Oracle
|===

=== Oracle VLDB Data Guard configuration with hypothetical NY to LA DR setup

[width=100%,cols="33%, 33%, 33%", frame=none, grid=rows]
|===
3+^| 
| *Database* | *DB_UNIQUE_NAME* | *Oracle Net Service Name*
| Primary | NTAP_NY | NTAP_NY.internal.cloudapp.net
| Standby | NTAP_LA | NTAP_LA.internal.cloudapp.net 
|===

=== Key factors for deployment consideration

* *Azure NetApp Files Configuration.* Azure NetApp Files are allocated in the Azure NetApp storage account as `Capacity Pools`. In these tests and validations, we deployed a 2 TiB capacity pool to host Oracle primary at the East region and a 4 TiB capacity pool to host standby database and DB clone at the West 2 region. ANF capacity pool has three service levels: Standard, Premium, and Ultra. The IO capacity of ANF capacity pool is based on the size of the capacity pool and its service level.  At a capacity pool creation, you can set QoS to Auto or Manual and data encryption at rest Single or Double. 

* *Sizing the Database Volumes.* For production deployment, NetApp recommends taking a full assessment of your Oracle database throughput requirement from Oracle AWR report. Take into consideration both the database size as well as the throughput requirements when size ANF volumes for database. With auto QoS configuration for ANF, the bandwidth is guaranteed at 128 MiB/s per TiB volume capacity allocated with Ultra Service Level. Higher throughput may need larger volume sizing to meet the requirement.    

* *Single Volume or Multiple Volumes.* A single large volume can provide similar performance level as multiple volumes with same aggregate size as the QoS is strictly enforced based on the volume sizing and capacity pool service level. It is recommended to implement multiple volumes (multiple NFS mount points) for Oracle VLDB to better utilize shared backend ANF storage resource pool. Implement Oracle ASM for IO load balancing on multiple NFS volumes.  

* *Azure VM Consideration.* In these tests and validations, we used an Azure VM - Standard_B4ms with 4 vCPUs and16 GiB memory. You need to choose the Azure DB VM appropriately for Oracle VLDB with high throughput requirement. Besides the number of vCPUs and the amount of RAM, the VM network bandwidth (ingress and egress ) can become a bottleneck before database storage capacity is reached.

* *dNFS Configuration.* By using dNFS, an Oracle database running on an Azure Virtual Machine with ANF storage can drive significantly more I/O than the native NFS client. Ensure that Oracle dNFS patch p32931941 is applied to address potential bugs. 

== Solution deployment

It is assumed that you already have your primary Oracle database deployed in an Azure cloud environment within a VNet as the starting point for setting up the Oracle Data Guard. Ideally, the primary database is deployed on ANF storage with NFS mount. Your primary Oracle database can also be running on a NetApp ONTAP storage or any other storage of choices either within the Azure ecosystem or a private data center. The following section demonstrates the configuration for Oracle VLDB on ANF in an Oracle Data Guard  setting between a primary Oracle DB in Azure with ANF storage to a physical standby Oracle DB in Azure with ANF storage.    

=== Prerequisites for deployment
[%collapsible]
====

Deployment requires the following prerequisites.

. An Azure cloud account has been set up, and the necessary VNet and network subnets have been created within your Azure account.

. From the Azure cloud portal console, you need to deploy minimum three Azure Linux VMs, one as the primary Oracle DB server, one as the standby Oracle DB server, and a clone target DB server for reporting, dev, and test etc. See the architecture diagram in the previous section for more details about the environment setup. Also review the Microsoft link:https://azure.microsoft.com/en-us/products/virtual-machines[Azure Virtual Machines^] for more information. 

. The primary Oracle database should have been installed and configured in the primary Oracle DB server. On the other hand, in the standby Oracle DB server or the clone Oracle DB server, only Oracle software is installed and no Oracle databases are created. Ideally, the Oracle files directories layout should be exactly matching on all Oracle DB servers. For details on NetApp recommendation for automated Oracle deployment in the Azure cloud and ANF, please refer to the following technical reports for help. 

* link:automation_ora_anf_nfs.html[TR-4987: Simplified, Automated Oracle Deployment on Azure NetApp Files with NFS^]
+
[NOTE]

Ensure that you have allocated at least 128G in the Azure VMs root volume in order to have sufficient space to stage Oracle installation files.

. From the Azure cloud portal console, deploy two ANF storage capacity pools to host Oracle database volumes. The ANF storage capacity pools should be situated in different regions to mimic a true DataGuard configuration. If you are not familiar with the deployment of ANF storage, see the documentation link:https://learn.microsoft.com/en-us/azure/azure-netapp-files/azure-netapp-files-quickstart-set-up-account-create-volumes?tabs=azure-portal[Quickstart: Set up Azure NetApp Files and create an NFS volume^] for step-by-step instructions.
+
image:azure_ora_anf_dg_anf_01.png["Screenshot showing Azure environment configuration."]

. When the primary Oracle database and the standby Oracle database are situated in two different regions, a VPN gateway should be configured to allow data traffic flow between two separate VNets. Detailed networking configuration in Azure is beyond the scope of this document. Following screen shots provides some reference on how the VPN gateways are configured, connected, and the data traffics flow are confirmed in the lab. 
+
Lab VPN gateways:
image:azure_ora_anf_dg_vnet_01.png["Screenshot showing Azure environment configuration."]
+
The primary vnet gateway:
image:azure_ora_anf_dg_vnet_02.png["Screenshot showing Azure environment configuration."]
+
Vnet gateway connection status:
image:azure_ora_anf_dg_vnet_03.png["Screenshot showing Azure environment configuration."]
+
Validate that the traffic flows are established (click on three dots to open the page):
image:azure_ora_anf_dg_vnet_04.png["Screenshot showing Azure environment configuration."]

====

=== Primary Oracle VLDB configuration for Data Guard
[%collapsible]

====

In this demonstration, we have setup a primary Oracle database called NTAP on the primary Azure DB server with three NFS mount points: /u01 for the Oracle binary, /u02 for the Oracle data files, and an Oracle control file, /u03 for the Oracle active logs, archived log files, and a redundant Oracle control file. Following illustrates the detailed procedures for setting up primary database for the Oracle Data Guard protection. All steps should be executed as the Oracle database owner or the default `oracle` user.

. The primary database NTAP on the primary Azure DB server orap.internal.cloudapp.net is initially deployed as a standalone database with the ANF on NFS and ASM as database storage.  
+
....

orap.internal.cloudapp.net:
resource group: ANFAVSRG
Location: East US
size: Standard B4ms (4 vcpus, 16 GiB memory)
OS: Linux (redhat 8.6)
pub_ip: 172.190.207.231
pri_ip: 10.0.0.4

[oracle@orap ~]$ df -h
Filesystem                 Size  Used Avail Use% Mounted on
devtmpfs                   7.7G     0  7.7G   0% /dev
tmpfs                      7.8G  1.1G  6.7G  15% /dev/shm
tmpfs                      7.8G   17M  7.7G   1% /run
tmpfs                      7.8G     0  7.8G   0% /sys/fs/cgroup
/dev/mapper/rootvg-rootlv   22G   20G  2.1G  91% /
/dev/mapper/rootvg-usrlv    10G  2.3G  7.8G  23% /usr
/dev/sda1                  496M  181M  315M  37% /boot
/dev/mapper/rootvg-varlv   8.0G  1.1G  7.0G  13% /var
/dev/sda15                 495M  5.8M  489M   2% /boot/efi
/dev/mapper/rootvg-homelv  2.0G   47M  2.0G   3% /home
/dev/mapper/rootvg-tmplv    12G   11G  1.9G  85% /tmp
/dev/sdb1                   32G   49M   30G   1% /mnt
10.0.2.38:/orap-u06        300G  282G   19G  94% /u06
10.0.2.38:/orap-u04        300G  282G   19G  94% /u04
10.0.2.36:/orap-u01        400G   21G  380G   6% /u01
10.0.2.37:/orap-u02        300G  282G   19G  94% /u02
10.0.2.36:/orap-u03        400G  282G  119G  71% /u03
10.0.2.39:/orap-u05        300G  282G   19G  94% /u05


[oracle@orap ~]$ cat /etc/oratab
#



# This file is used by ORACLE utilities.  It is created by root.sh
# and updated by either Database Configuration Assistant while creating
# a database or ASM Configuration Assistant while creating ASM instance.

# A colon, ':', is used as the field terminator.  A new line terminates
# the entry.  Lines beginning with a pound sign, '#', are comments.
#
# Entries are of the form:
#   $ORACLE_SID:$ORACLE_HOME:<N|Y>:
#
# The first and second fields are the system identifier and home
# directory of the database respectively.  The third field indicates
# to the dbstart utility that the database should , "Y", or should not,
# "N", be brought up at system boot time.
#
# Multiple entries with the same $ORACLE_SID are not allowed.
#
#
+ASM:/u01/app/oracle/product/19.0.0/grid:N
NTAP:/u01/app/oracle/product/19.0.0/NTAP:N



....

. Login to primary DB server as the oracle user. Validate grid configuration.
+
[source, cli]
$GRID_HOME/bin/crsctl stat res -t
+
....
[oracle@orap ~]$ $GRID_HOME/bin/crsctl stat res -t
--------------------------------------------------------------------------------
Name           Target  State        Server                   State details
--------------------------------------------------------------------------------
Local Resources
--------------------------------------------------------------------------------
ora.DATA.dg
               ONLINE  ONLINE       orap                     STABLE
ora.LISTENER.lsnr
               ONLINE  ONLINE       orap                     STABLE
ora.LOGS.dg
               ONLINE  ONLINE       orap                     STABLE
ora.asm
               ONLINE  ONLINE       orap                     Started,STABLE
ora.ons
               OFFLINE OFFLINE      orap                     STABLE
--------------------------------------------------------------------------------
Cluster Resources
--------------------------------------------------------------------------------
ora.cssd
      1        ONLINE  ONLINE       orap                     STABLE
ora.diskmon
      1        OFFLINE OFFLINE                               STABLE
ora.evmd
      1        ONLINE  ONLINE       orap                     STABLE
ora.ntap.db
      1        OFFLINE OFFLINE                               Instance Shutdown,ST
                                                             ABLE
--------------------------------------------------------------------------------
[oracle@orap ~]$

....

. ASM disk group configuration.
+
[source, cli]
asmcmd
+
....

[oracle@orap ~]$ asm
[oracle@orap ~]$ asmcmd
ASMCMD> lsdg
State    Type    Rebal  Sector  Logical_Sector  Block       AU  Total_MB  Free_MB  Req_mir_free_MB  Usable_file_MB  Offline_disks  Voting_files  Name
MOUNTED  EXTERN  N         512             512   4096  4194304   1146880  1136944                0         1136944              0             N  DATA/
MOUNTED  EXTERN  N         512             512   4096  4194304    286720   283312                0          283312              0             N  LOGS/
ASMCMD> lsdsk
Path
/u02/oradata/asm/orap_data_disk_01
/u02/oradata/asm/orap_data_disk_02
/u02/oradata/asm/orap_data_disk_03
/u02/oradata/asm/orap_data_disk_04
/u03/oralogs/asm/orap_logs_disk_01
/u03/oralogs/asm/orap_logs_disk_02
/u03/oralogs/asm/orap_logs_disk_03
/u03/oralogs/asm/orap_logs_disk_04
/u04/oradata/asm/orap_data_disk_05
/u04/oradata/asm/orap_data_disk_06
/u04/oradata/asm/orap_data_disk_07
/u04/oradata/asm/orap_data_disk_08
/u05/oradata/asm/orap_data_disk_09
/u05/oradata/asm/orap_data_disk_10
/u05/oradata/asm/orap_data_disk_11
/u05/oradata/asm/orap_data_disk_12
/u06/oradata/asm/orap_data_disk_13
/u06/oradata/asm/orap_data_disk_14
/u06/oradata/asm/orap_data_disk_15
/u06/oradata/asm/orap_data_disk_16
ASMCMD>

....



====

=== Standby Oracle VLDB configuration for Data Guard
[%collapsible]

====

Oracle Data Guard requires OS kernel configuration and Oracle software stacks including patch sets on standby DB server to match with primary DB server. For easy management and simplicity, the database storage configuration of the standby DB server ideally should match with the primary DB server as well, such as the database directory layout and sizes of NFS mount points. Following are detail procedures for setting up the standby Oracle DB server and activating the Oracle DataGuard for HA/DR protection. All commands should be executed as the default Oracle owner user id `oracle`.

. First, review the configuration of the primary database on primary Oracle DB server. In this demonstration, we have setup a primary Oracle database called NTAP in the primary DB server with three NFS mounts on ANF storage. 


. If you follow the NetApp documemntation TR-4987 to setup the Oracle standby DB server link:automation_ora_anf_nfs.html[TR-4987: Simplified, Automated Oracle Deployment on Azure NetApp Files with NFS^], use a tag `-t software_only_install` in step 2 of `Playbook execution` to run automated Oracle installation. The revised command syntax is listed below. The tag will allow the Oracle software stack installed and configured but stop short of creating a database.
+
[source, cli]
ansible-playbook -i hosts 4-oracle_config.yml -u azureuser -e @vars/vars.yml -t software_only_install

. The standby Oracle DB server configuration at standby site in the demo lab. 
+
....
oras.internal.cloudapp.net:
resource group: ANFAVSRG
Location: West US 2
size: Standard B4ms (4 vcpus, 16 GiB memory)
OS: Linux (redhat 8.6)
pub_ip: 172.179.119.75
pri_ip: 10.0.1.4

[oracle@oras ~]$ df -h
Filesystem                 Size  Used Avail Use% Mounted on
devtmpfs                   7.7G     0  7.7G   0% /dev
tmpfs                      7.8G  1.1G  6.7G  15% /dev/shm
tmpfs                      7.8G   25M  7.7G   1% /run
tmpfs                      7.8G     0  7.8G   0% /sys/fs/cgroup
/dev/mapper/rootvg-rootlv   22G   17G  5.6G  75% /
/dev/mapper/rootvg-usrlv    10G  2.3G  7.8G  23% /usr
/dev/mapper/rootvg-varlv   8.0G  1.1G  7.0G  13% /var
/dev/mapper/rootvg-homelv  2.0G   52M  2.0G   3% /home
/dev/sda1                  496M  181M  315M  37% /boot
/dev/sda15                 495M  5.8M  489M   2% /boot/efi
/dev/mapper/rootvg-tmplv    12G   11G  1.8G  86% /tmp
/dev/sdb1                   32G   49M   30G   1% /mnt
10.0.3.36:/oras-u03        400G  282G  119G  71% /u03
10.0.3.36:/oras-u04        300G  282G   19G  94% /u04
10.0.3.36:/oras-u05        300G  282G   19G  94% /u05
10.0.3.36:/oras-u02        300G  282G   19G  94% /u02
10.0.3.36:/oras-u01        100G   21G   80G  21% /u01
10.0.3.36:/oras-u06        300G  282G   19G  94% /u06


....

. Once Oracle software is installed and configured, set oracle home and path. Also, from the standby $ORACLE_HOME dbs directory, copy oracle password from primary database if you have not done so.
+
[source, cli]
export ORACLE_HOME=/u01/app/oracle/product/19.0.0/NTAP
+
[source, cli]
export PATH=$PATH:$ORACLE_HOME/bin
+
[source, cli]
scp oracle@10.0.0.4:$ORACLE_HOME/dbs/orapwNTAP .

. Update tnsnames.ora file with following entries.
+
[source, cli]
vi $ORACLE_HOME/network/admin/tnsnames.ora
+
....

# tnsnames.ora Network Configuration File: /u01/app/oracle/product/19.0.0/NTAP/network/admin/tnsnames.ora
# Generated by Oracle configuration tools.

NTAP_NY =
  (DESCRIPTION =
    (ADDRESS = (PROTOCOL = TCP)(HOST = orap.internal.cloudapp.net)(PORT = 1521))
    (CONNECT_DATA =
      (SERVER = DEDICATED)
      (SID = NTAP)
    )
  )

NTAP_LA =
  (DESCRIPTION =
    (ADDRESS = (PROTOCOL = TCP)(HOST = oras.internal.cloudapp.net)(PORT = 1521))
    (CONNECT_DATA =
      (SERVER = DEDICATED)
      (SID = NTAP)
    )
  )


....

. Grid infrastructure configuration.
+
[source, cli]
$GRID_HOME/bin/crsctl stat res -t
+
....

[oracle@oras ~]$ $GRID_HOME/bin/crsctl stat res -t
--------------------------------------------------------------------------------
Name           Target  State        Server                   State details
--------------------------------------------------------------------------------
Local Resources
--------------------------------------------------------------------------------
ora.DATA.dg
               ONLINE  ONLINE       oras                     STABLE
ora.LISTENER.lsnr
               ONLINE  ONLINE       oras                     STABLE
ora.LOGS.dg
               ONLINE  ONLINE       oras                     STABLE
ora.asm
               ONLINE  ONLINE       oras                     Started,STABLE
ora.ons
               OFFLINE OFFLINE      oras                     STABLE
--------------------------------------------------------------------------------
Cluster Resources
--------------------------------------------------------------------------------
ora.cssd
      1        ONLINE  ONLINE       oras                     STABLE
ora.diskmon
      1        OFFLINE OFFLINE                               STABLE
ora.evmd
      1        ONLINE  ONLINE       oras                     STABLE
ora.ntap_la.db
      1        ONLINE  INTERMEDIATE oras                     Dismounted,Mount Ini
                                                             tiated,HOME=/u01/app
                                                             /oracle/product/19.0
                                                             .0/NTAP,STABLE
--------------------------------------------------------------------------------
[oracle@oras ~]$

....



. ASM configuration.
+
[source, cli]
asmcmd
+
....

[oracle@oras ~]$ asmcmd
ASMCMD> lsdg
State    Type    Rebal  Sector  Logical_Sector  Block       AU  Total_MB  Free_MB  Req_mir_free_MB  Usable_file_MB  Offline_disks  Voting_files  Name
MOUNTED  EXTERN  N         512             512   4096  4194304   1146880  1136912                0         1136912              0             N  DATA/
MOUNTED  EXTERN  N         512             512   4096  4194304    286720   284228                0          284228              0             N  LOGS/
ASMCMD> lsdsk
Path
/u02/oradata/asm/oras_data_disk_01
/u02/oradata/asm/oras_data_disk_02
/u02/oradata/asm/oras_data_disk_03
/u02/oradata/asm/oras_data_disk_04
/u03/oralogs/asm/oras_logs_disk_01
/u03/oralogs/asm/oras_logs_disk_02
/u03/oralogs/asm/oras_logs_disk_03
/u03/oralogs/asm/oras_logs_disk_04
/u04/oradata/asm/oras_data_disk_05
/u04/oradata/asm/oras_data_disk_06
/u04/oradata/asm/oras_data_disk_07
/u04/oradata/asm/oras_data_disk_08
/u05/oradata/asm/oras_data_disk_09
/u05/oradata/asm/oras_data_disk_10
/u05/oradata/asm/oras_data_disk_11
/u05/oradata/asm/oras_data_disk_12
/u06/oradata/asm/oras_data_disk_13
/u06/oradata/asm/oras_data_disk_14
/u06/oradata/asm/oras_data_disk_15
/u06/oradata/asm/oras_data_disk_16
ASMCMD>


....

. Validate the duplicated standby database. Newly duplicated standby database open in READ ONLY mode initially.
+
....

[oracle@oras admin]$ cat /etc/oratab
#



# This file is used by ORACLE utilities.  It is created by root.sh
# and updated by either Database Configuration Assistant while creating
# a database or ASM Configuration Assistant while creating ASM instance.

# A colon, ':', is used as the field terminator.  A new line terminates
# the entry.  Lines beginning with a pound sign, '#', are comments.
#
# Entries are of the form:
#   $ORACLE_SID:$ORACLE_HOME:<N|Y>:
#
# The first and second fields are the system identifier and home
# directory of the database respectively.  The third field indicates
# to the dbstart utility that the database should , "Y", or should not,
# "N", be brought up at system boot time.
#
# Multiple entries with the same $ORACLE_SID are not allowed.
#
#
NTAP:/u01/app/oracle/product/19.0.0/NTAP:N
[oracle@oras admin]$ export ORACLE_SID=NTAP
[oracle@oras admin]$ sqlplus / as sysdba

SQL*Plus: Release 19.0.0.0.0 - Production on Tue Nov 26 23:04:07 2024
Version 19.18.0.0.0

Copyright (c) 1982, 2022, Oracle.  All rights reserved.


Connected to:
Oracle Database 19c Enterprise Edition Release 19.0.0.0.0 - Production
Version 19.18.0.0.0

SQL> select name, open_mode from v$database;

NAME      OPEN_MODE
--------- --------------------
NTAP      READ ONLY

SQL> show parameter name

NAME                                 TYPE        VALUE
------------------------------------ ----------- ------------------------------
cdb_cluster_name                     string
cell_offloadgroup_name               string
db_file_name_convert                 string
db_name                              string      NTAP
db_unique_name                       string      NTAP_LA
global_names                         boolean     FALSE
instance_name                        string      NTAP
lock_name_space                      string
log_file_name_convert                string
pdb_file_name_convert                string
processor_group_name                 string

NAME                                 TYPE        VALUE
------------------------------------ ----------- ------------------------------
service_names                        string      NTAP_LA.internal.cloudapp.net
SQL> show parameter log_archive_config

NAME                                 TYPE        VALUE
------------------------------------ ----------- ------------------------------
log_archive_config                   string      DG_CONFIG=(NTAP_NY,NTAP_LA)
SQL> show parameter fal_server

NAME                                 TYPE        VALUE
------------------------------------ ----------- ------------------------------
fal_server                           string      NTAP_NY
SQL> select name from v$datafile;

NAME
--------------------------------------------------------------------------------
/u02/oradata/NTAP/system01.dbf
/u02/oradata/NTAP/sysaux01.dbf
/u02/oradata/NTAP/undotbs01.dbf
/u02/oradata/NTAP/pdbseed/system01.dbf
/u02/oradata/NTAP/pdbseed/sysaux01.dbf
/u02/oradata/NTAP/users01.dbf
/u02/oradata/NTAP/pdbseed/undotbs01.dbf
/u02/oradata/NTAP/NTAP_pdb1/system01.dbf
/u02/oradata/NTAP/NTAP_pdb1/sysaux01.dbf
/u02/oradata/NTAP/NTAP_pdb1/undotbs01.dbf
/u02/oradata/NTAP/NTAP_pdb1/users01.dbf

NAME
--------------------------------------------------------------------------------
/u02/oradata/NTAP/NTAP_pdb2/system01.dbf
/u02/oradata/NTAP/NTAP_pdb2/sysaux01.dbf
/u02/oradata/NTAP/NTAP_pdb2/undotbs01.dbf
/u02/oradata/NTAP/NTAP_pdb2/users01.dbf
/u02/oradata/NTAP/NTAP_pdb3/system01.dbf
/u02/oradata/NTAP/NTAP_pdb3/sysaux01.dbf
/u02/oradata/NTAP/NTAP_pdb3/undotbs01.dbf
/u02/oradata/NTAP/NTAP_pdb3/users01.dbf

19 rows selected.

SQL> select name from v$controlfile;

NAME
--------------------------------------------------------------------------------
/u02/oradata/NTAP/control01.ctl
/u03/orareco/NTAP_LA/control02.ctl

SQL> col member form a80
SQL> select group#, type, member from v$logfile order by 2, 1;

    GROUP# TYPE    MEMBER
---------- ------- --------------------------------------------------------------------------------
         1 ONLINE  /u03/orareco/NTAP_LA/onlinelog/o1_mf_1_mndl6mxh_.log
         2 ONLINE  /u03/orareco/NTAP_LA/onlinelog/o1_mf_2_mndl7jdb_.log
         3 ONLINE  /u03/orareco/NTAP_LA/onlinelog/o1_mf_3_mndl8f03_.log
         4 STANDBY /u03/orareco/NTAP_LA/onlinelog/o1_mf_4_mndl99m7_.log
         5 STANDBY /u03/orareco/NTAP_LA/onlinelog/o1_mf_5_mndlb67d_.log
         6 STANDBY /u03/orareco/NTAP_LA/onlinelog/o1_mf_6_mndlc2tw_.log
         7 STANDBY /u03/orareco/NTAP_LA/onlinelog/o1_mf_7_mndlczhb_.log

7 rows selected.


....

. Restart the standby database in `mount` stage and execute following command to activate standby database managed recovery.
+
[source, cli]
alter database recover managed standby database disconnect from session;
+
....

SQL> shutdown immediate;
Database closed.
Database dismounted.
ORACLE instance shut down.
SQL> startup mount;
ORACLE instance started.

Total System Global Area 6442449688 bytes
Fixed Size                  9177880 bytes
Variable Size            1090519040 bytes
Database Buffers         5335154688 bytes
Redo Buffers                7598080 bytes
Database mounted.
SQL> alter database recover managed standby database disconnect from session;

Database altered.

....

. Validate the standby database recovery status. Notice the `recovery logmerger` in `APPLYING_LOG` action.
+
[source, cli]
SELECT ROLE, THREAD#, SEQUENCE#, ACTION FROM V$DATAGUARD_PROCESS;
....

SQL> SELECT ROLE, THREAD#, SEQUENCE#, ACTION FROM V$DATAGUARD_PROCESS;

ROLE                        THREAD#  SEQUENCE# ACTION
------------------------ ---------- ---------- ------------
post role transition              0          0 IDLE
recovery apply slave              0          0 IDLE
recovery apply slave              0          0 IDLE
recovery apply slave              0          0 IDLE
recovery apply slave              0          0 IDLE
recovery logmerger                1         18 APPLYING_LOG
managed recovery                  0          0 IDLE
RFS async                         1         18 IDLE
RFS ping                          1         18 IDLE
archive redo                      0          0 IDLE
redo transport timer              0          0 IDLE

ROLE                        THREAD#  SEQUENCE# ACTION
------------------------ ---------- ---------- ------------
gap manager                       0          0 IDLE
archive redo                      0          0 IDLE
archive redo                      0          0 IDLE
redo transport monitor            0          0 IDLE
log writer                        0          0 IDLE
archive local                     0          0 IDLE

17 rows selected.

SQL>


....

This completes the Data Guard protection setup for NTAP from primary to standby with managed standby recovery enabled.

====


=== Setup Data Guard Broker 
[%collapsible]
 
====

Oracle Data Guard broker is a distributed management framework that automates and centralizes the creation, maintenance, and monitoring of Oracle Data Guard configurations. Following section demonstrate how to setup Data Guard Broker to manage Data Guard environment.

. Start data guard broker on both the primary and the standby databases with following command via sqlplus.
+
[source, cli]
alter system set dg_broker_start=true scope=both;

. From primary database, connect to Data Guard Borker as SYSDBA.
+
....

[oracle@orap ~]$ dgmgrl sys@NTAP_NY
DGMGRL for Linux: Release 19.0.0.0.0 - Production on Wed Dec 11 20:53:20 2024
Version 19.18.0.0.0

Copyright (c) 1982, 2019, Oracle and/or its affiliates.  All rights reserved.

Welcome to DGMGRL, type "help" for information.
Password:
Connected to "NTAP_NY"
Connected as SYSDBA.
DGMGRL>


....

. Create and enable Data Guard Broker configuration.
+
....

DGMGRL> create configuration dg_config as primary database is NTAP_NY connect identifier is NTAP_NY;
Configuration "dg_config" created with primary database "ntap_ny"
DGMGRL> add database NTAP_LA as connect identifier is NTAP_LA;
Database "ntap_la" added
DGMGRL> enable configuration;
Enabled.
DGMGRL> show configuration;

Configuration - dg_config

  Protection Mode: MaxPerformance
  Members:
  ntap_ny - Primary database
    ntap_la - Physical standby database

Fast-Start Failover:  Disabled

Configuration Status:
SUCCESS   (status updated 3 seconds ago)

....

. Validate the database status within the Data Guard Broker management framework.
+
....

DGMGRL> show database db1_ny;

Database - db1_ny

  Role:               PRIMARY
  Intended State:     TRANSPORT-ON
  Instance(s):
    db1

Database Status:
SUCCESS

DGMGRL> show database db1_la;

Database - db1_la

  Role:               PHYSICAL STANDBY
  Intended State:     APPLY-ON
  Transport Lag:      0 seconds (computed 1 second ago)
  Apply Lag:          0 seconds (computed 1 second ago)
  Average Apply Rate: 2.00 KByte/s
  Real Time Query:    OFF
  Instance(s):
    db1

Database Status:
SUCCESS

DGMGRL>

....

In the event of a failure, Data Guard Broker can be used to failover the primary database to the standby instantaniouly. If `Fast-Start Failover` is enabled, Data Guard Broker can failover the primary database to the standby when a failure is detected without an user intervention.

====

=== Clone standby databse for other use cases 
[%collapsible]

====

The key benefit of hosting the Oracle standby database on the ANF in the Oracle Data Guard setup is that it can be quickly cloned to serve many other use cases with minimal additional storage investment if a thin clone is enabled. NetApp recommends to use SnapCenter UI tool to manage your Oracle DataGuard database. In the following section, we demonstrate how to snapshot and clone the mounted and under recovery standby database volumes on the ANF for other purposes, such as DEV, TEST, REPORT, etc., using the NetApp SnapCenter tool.

Below are high level procedures to clone a READ/WRITE database from the managed physical standby database in the Oracle Data Guard using SnapCenter. For detail instructions on how to setup and configure SnapCenter for Oracle on ANF, please refer to TR-4988 link:snapctr_ora_azure_anf.html[Oracle Database Backup, Recovery, and Clone on ANF with SnapCenter^] for details. 

. We begin the usecase validation by creating a test table and inserting a row into the test table at the primary database. We will then validate that the transaction traverses down to standby and finally the clone. 
+
....
[oracle@orap ~]$ sqlplus / as sysdba

SQL*Plus: Release 19.0.0.0.0 - Production on Wed Dec 11 16:33:17 2024
Version 19.18.0.0.0

Copyright (c) 1982, 2022, Oracle.  All rights reserved.


Connected to:
Oracle Database 19c Enterprise Edition Release 19.0.0.0.0 - Production
Version 19.18.0.0.0

SQL> alter session set container=ntap_pdb1;

Session altered.

SQL> create table test(id integer, dt timestamp, event varchar(100));

Table created.

SQL> insert into test values(1, sysdate, 'a test transaction at primary database NTAP on DB server orap.internal.cloudapp.net');

1 row created.

SQL> commit;

Commit complete.

SQL> select * from test;

        ID
----------
DT
---------------------------------------------------------------------------
EVENT
--------------------------------------------------------------------------------
         1
11-DEC-24 04.38.44.000000 PM
a test transaction at primary database NTAP on DB server orap.internal.cloudapp.
net


SQL> select instance_name, host_name from v$instance;

INSTANCE_NAME
----------------
HOST_NAME
----------------------------------------------------------------
NTAP
orap


SQL>

....

. In SnapCenter configuration, an unix user (azureuser for demo) and an Azure credential (azure_anf for demo) has been added to `Credential` in `Settings`.
+
image:azure_ora_anf_dg_snapctr_config_17.png["Screenshot showing this step in the GUI."]

. Use azure_anf credential to add the ANF storage to `Storage Systems`. If you have multiple ANF storage accounts in your Azure subsciption, make sure click the drop down list to choose the right storage account. We have created two dedicated Oracle storage accounts for this demonstration. 
+
image:azure_ora_anf_dg_snapctr_config_16.png["Screenshot showing this step in the GUI."]

. All Oracle DB servers have been added to SnapCenter `Hosts`.
+
image:azure_ora_anf_dg_snapctr_config_18.png["Screenshot showing this step in the GUI."]
+
[NOTE]

The clone DB server should have identtical Oracle software stacks installed and configured. In our test case, Oracle 19C software is installed and configured but no database created. 

. Create a backup policy that is tailored for offline/mount full database backup.
+
image:azure_ora_anf_dg_snapctr_bkup_08.PNG["Screenshot showing this step in the GUI."]

. Apply backup policy to protect standby database in `Resources` tab. When initially discovered, the database status shows as `Not protected`. 
+
image:azure_ora_anf_dg_snapctr_bkup_09.PNG["Screenshot showing this step in the GUI."]

. You have option to either trigger a backup manually or put it on a schedule at a set time after a backup policy applied. 
+
image:azure_ora_anf_dg_snapctr_bkup_15.PNG["Screenshot showing this step in the GUI."]

. After a backup, click on database name to open the database backups page. Select a backup to be used for database clone and click on `Clone` button to launch clone workflow. 
+
image:azure_ora_anf_dg_snapctr_clone_01.png["Screenshot showing this step in the GUI."]

. Select the `Complete Database Clone` and name the clone instance SID.
+
image:azure_ora_anf_dg_snapctr_clone_02.png["Screenshot showing this step in the GUI."]

. Select the clone DB server, which hosts the cloned database from the standby DB. Accept the default for data files, redo logs. Put a controlfile on /u03 mount point. 
+
image:azure_ora_anf_dg_snapctr_clone_03.png["Screenshot showing this step in the GUI."]

. No database credentials are needed for OS based authentication. Match Oracle home setting with what is configured on the clone DB server. 
+
image:azure_ora_anf_dg_snapctr_clone_04.png["Screenshot showing this step in the GUI."]

. Change clone database parameters if needed such as lowering PGA or SGA size for a clone DB. Specify scripts to run before the clone if any.
+
image:azure_ora_anf_dg_snapctr_clone_05.png["Screenshot showing this step in the GUI."]

. Enter SQL to run after the clone. In the demo, we executed commands to turn off database archive mode for a dev/test/report database. 
+
image:azure_ora_anf_dg_snapctr_clone_06_1.png["Screenshot showing this step in the GUI."]

. Configure email notification if desired.
+
image:azure_ora_anf_dg_snapctr_clone_07.png["Screenshot showing this step in the GUI."]

. Review the summary, click `Finish` to start the clone.
+
image:azure_ora_anf_dg_snapctr_clone_08.png["Screenshot showing this step in the GUI."]

. Monitor the clone job in `Monitor` tab. We observed that it took around 14 minutes to clone a database about 950GB in database volume size.
+
image:azure_ora_anf_dg_snapctr_clone_09.png["Screenshot showing this step in the GUI."]

. Validate the clone database from SnapCenter, which is immediately registered in `Resources` tab right after clone operation.
+
image:azure_ora_anf_dg_snapctr_clone_10.png["Screenshot showing this step in the GUI."]

. Query the clone database from clone DB server. We validated that test transaction that occurred in primary database had traversed down to the clone database. 
+
....
[oracle@orac ~]$ sqlplus / as sysdba

SQL*Plus: Release 19.0.0.0.0 - Production on Wed Dec 11 20:16:09 2024
Version 19.18.0.0.0

Copyright (c) 1982, 2022, Oracle.  All rights reserved.


Connected to:
Oracle Database 19c Enterprise Edition Release 19.0.0.0.0 - Production
Version 19.18.0.0.0

SQL> select name, open_mode, log_mode from v$database;

NAME      OPEN_MODE            LOG_MODE
--------- -------------------- ------------
NTAPDEV   READ WRITE           NOARCHIVELOG

SQL> select instance_name, host_name from v$instance;

INSTANCE_NAME
----------------
HOST_NAME
----------------------------------------------------------------
NTAPDEV
orac


SQL> alter pluggable database all open;

Pluggable database altered.

SQL> alter pluggable database all save state;

Pluggable database altered.


SQL> alter session set container=ntap_pdb1;

Session altered.

SQL> select * from test;

        ID
----------
DT
---------------------------------------------------------------------------
EVENT
--------------------------------------------------------------------------------
         1
11-DEC-24 04.38.44.000000 PM
a test transaction at primary database NTAP on DB server orap.internal.cloudapp.
net


....


This completes the demonstration of the Oracle standby database clone in the Oracle Data Guard on Azure ANF storage for DEV, TEST, REPORT, or any other use cases. Multiple Oracle databases can be cloned off the same standby database in the Oracle Data Guard on ANF.


====


== Where to find additional information

To learn more about the information described in this document, review the following documents and/or websites:

* Azure NetApp Files
+
link:https://azure.microsoft.com/en-us/products/netapp[https://azure.microsoft.com/en-us/products/netapp^]


* TR-4988: Oracle Database Backup, Recovery, and Clone on ANF with SnapCenter
+
link:https://docs.netapp.com/us-en/netapp-solutions/databases/snapctr_ora_azure_anf.html[https://docs.netapp.com/us-en/netapp-solutions/databases/snapctr_ora_azure_anf.html^]

* TR-4987: Simplified, Automated Oracle Deployment on Azure NetApp Files with NFS
+
link:https://docs.netapp.com/us-en/netapp-solutions/databases/automation_ora_anf_nfs.html[https://docs.netapp.com/us-en/netapp-solutions/databases/automation_ora_anf_nfs.html^]

* Oracle Data Guard Concepts and Administration
+
link:https://docs.oracle.com/en/database/oracle/oracle-database/19/sbydb/index.html#Oracle%C2%AE-Data-Guard[https://docs.oracle.com/en/database/oracle/oracle-database/19/sbydb/index.html#Oracle%C2%AE-Data-Guard^]





