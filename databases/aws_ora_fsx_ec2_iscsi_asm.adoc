---
sidebar: sidebar
permalink: databases/aws_ora_fsx_ec2_iscsi_asm.html
keywords: Oracle, AWS, FSx ONTAP, Database, Oracle ASM, Oracle Restart, iSCSI
summary: "The solution provides overview and details for Oracle database deployment and protection in AWS FSx ONTAP storage and EC2 compute instance with iSCSI protocol and Oracle database configured in standalone ReStart using asm as volume manager." 
---

= TR-XXXX: Oracle Database Deployment and Protection in AWS FSx/EC2 with iSCSI/ASM
:hardbreaks:
:nofooter:
:icons: font
:linkattrs:
:imagesdir: ./../media/

Allen Cao, Niyaz Mohamed, NetApp

[.lead]

== Purpose

ASM - automatic storage management is a popular Oracle storage volume manager that is employed in many Oracle installations. It is also the Oracle's recommended storage management solution. It provides an alternative to conventional volume managers and file systems. Since Oracle version 11g, ASM has been packaged with grid infrastructure rather than database. As a result, in order to utilize Oracle ASM for storage management without RAC, Oracle grid infrastructure can be installed in a Standalone server, also known as Oracle Restart. It certainly adds more complexity in an otherwise simpler Oracle database deployment. However, as the name implies, when Oracle is deployed in Restart mode, it gains the capability to restart all failed Oracle services or after a host reboot without user intervention. It achieves certain degree of high availability or HA functionality.

In this documentation, we demonstrate how you can deploy Oracle database with iSCSI protocol and Oracle ASM in AWS FSx ONTAP storage and EC2 compute instances environment step by step. We will also demonstrate how you could use NetApp SnapCenter service through NetApp BlueXP console to backup, restore, and clone your Oracle database for DEV/TEST or any other use cases for storage efficient database operation in AWS public cloud.  

This solution addresses the following use cases:

* Oracle database deployment in AWS FSx ONTAP storage and EC2 compute instances with iSCSI/ASM 
* Testing and validating a Oracle workload in the public AWS cloud with iSCSI/ASM
* Testing and validating a Oracle database Restart functionalities deployed in AWS

== Audience

This solution is intended for the following people:

* The DBA who is interested in deploying Oracle in AWS public cloud with iSCSI/ASM.
* The database solution architect who is interested in testing Oracle workloads in the public AWS cloud.
* The storage administrator who is interested in deploying and managing Oracle database deployed to AWS FSx storage.
* The application owner who is interested in standing up an Oracle database in AWS FSx/EC2.

== Solution test and validation environment

The testing and validation of this solution was performed in an AWS FSx and EC2 environment that might not match the final deployment environment. For more information, see the section <<Key Factors for Deployment Consideration>>.

=== Architecture

image::aws_ora_fsx_ec2_iscsi_asm_architecture.png["This image provides a detailed picture of the Oracle deployment configuration in AWS public cloud with iSCSI and ASM."]

=== Hardware and software components

[%autowidth.stretch]
|===
3+^| *Hardware*
| FSx ONTAP storage | Current version offered by AWS | One FSx HA cluster in the same VPC and availability zone
| EC2 instance for compute | t2.xlarge/4vCPU/16G | Two EC2 T2 xlarge EC2 instances, one as primary DB server and the other as clone DB server 

3+^| *Software*
| RedHat Linux | RHEL-8.6.0_HVM-20220503-x86_64-2-Hourly2-GP2 | Deployed RedHat subscription for testing
| Oracle Grid Infrastructure | Version 19.18 | Applied RU patch p34762026_190000_Linux-x86-64.zip
| Oracle Database | Version 19.18 | Applied RU patch p34765931_190000_Linux-x86-64.zip
| Oracle OPatch | Version 12.2.0.1.36 | Latest patch p6880880_190000_Linux-x86-64.zip
| SnapCenter Service | Version |  
|===

=== Key factors for deployment consideration

* *EC2 compute instances.* In these tests and validations, we used the AWS EC2 t2.xlarge instance type for the Oracle database compute instance. NetApp recommends using an M5 type EC2 instance as the compute instance for Oracle in production deployment because it is optimized for database workloads. You need to size the EC2 instance appropriately in terms of number of vCPU and RAM based on actual workload requirements.

* *FSx storage HA clusters single- or multi-zone deployment.* In these tests and validations, we deployed an FSx HA cluster in a single AWS availability zone. For production deployment, NetApp recommends deploying an FSx HA pair in two different availability zones. An FSx HA cluster is alway provisioned in a HA pair that is sync mirrored in a pair of active-passive file systems to provide storage-level redundancy. Multi zones deployment further enhances high availability in the event of failure in a single AWS zone. 

* *FSx storage cluster sizing.* A FSx ONTAP storage file system provides up to 160,000 raw SSD IOPS, up to 4GB/s throughput, and maximum 192 TiB capacity. However, you can size the cluster in terms of provisioned IOPS, throughput, and storage limit (minimum 1,024 GiB) totally based on your actually requirements at the time of deployment. The capacity can be adjusted dynamically on the fly without any impact to application availability.   

* *Oracle data and logs layout.* In our tests and validations, we have deployed two ASM disk groups for data and logs respectively. Within +DATA asm disk group, we provisioned 4 luns in a data volume. Within +LOGS asm disk group, we provisioned two luns in a logs volume. In general, multiple luns layout within a FSx ONTAP volume provides better performance. 

* *iSCSI configuration.* The EC2 instance DB server connects to FSx storage via iSCSI protocol. EC2 instances generally deploy with a single network interface or ENI. The single NIC interface will carry both iSCSI and application traffic. It is important to gauge Oracle database peek IO throughput requirement by carefully analyzing Oracle AWR report in order to choose a right EC2 compute instance that meets both application and iSCSI traffic throughput requirement. NetApp also recommends to allocate 4 iSCSI connections to both FSx iSCSI endpoints with multipath properly configured.  

* *Oracle ASM redundancy level to use for each Oracle ASM disk group that you create.* Since FSx already mirrors the storage on the FSx cluster level, you should use External Redundancy, which means that the option does not allow Oracle ASM to mirror the contents of the disk group.

* *Database backup.* NetApp provides a SaaS version of SnapCenter software service for database backup, restore, and clone in cloud that is available via NetApp BlueXP console UI. It is recommended to implement such service to achieve fast (under a minute) SnapShot backup, quick (few minutes) database restore, and database clone.    

== Solution Deployment

The following provides the outline and details of step by step deployment procedures. 

=== Prerequisites for deployment
[%collapsible]
====

Deployment requires the following prerequisites.

. An AWS account has been set up, and the necessary VPC and network segments have been created within your AWS account.

. From the AWS EC2 console, you need to deploy two EC2 Linux instances, one as the primary Oracle DB server and an optional alternative clone target DB server. See the architecture diagram in the previous section for more details about the environment setup. Also review the link:https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/concepts.html[User Guide for Linux instances] for more information.

. From the AWS EC2 console, deploy a FSx ONTAP storage HA clusters to host the Oracle database volumes. If you are not familiar with the deployment of FSx storage, see the documentation link:https://docs.aws.amazon.com/fsx/latest/ONTAPGuide/creating-file-systems.html[Creating FSx for ONTAP file systems] for step-by-step instructions.

. The above step 2 and 3 can be setup using following Terraform automation toolkit, which will create an EC2 instance named ora_01 and a FSx file system named as fsx_01. Review the instruction carefully and change the variables to your environment specifics before execution.
+
[source, cli]
git clone https://github.com/NetApp-Automation/na_aws_fsx_ec2_deploy.git

====

=== EC2 instance kernel configuration
[%collapsible]

====
With prerequisites provisioned, login into EC2 instance as root user to configure linux kernel for Oracle installation.

. From Oracle, download and install Oracle 19c preinstall RPM, which will satisfy most kernel configuration requirement
+
[source, cli]
yum install oracle-database-preinstall-19c-1.0-2.el8.x86_64.rpm

. Download and install missing compat-libcap1 in Linux 8
+
[source, cli]
yum install compat-libcap1-1.10-7.el7.x86_64.rpm

. From NetApp, download and install NetApp host utilities 
+
[source, cli]
yum install netapp_linux_unified_host_utilities-7-1.x86_64.rpm

. Install open JDK version 1.8
+
[source, cli]
yum install java-1.8.0-openjdk.x86_64

. Install iSCSI initiator utils
+
[source, cli]
yum install iscsi-initiator-utils

. Install sg3_utils
+
[source, cli]
yum install sg3_utils

. Install device-mapper-multipath
+
[source, cli]
yum install device-mapper-multipath

. Disable transparent hugepages in current system
+
[source, cli]
echo never > /sys/kernel/mm/transparent_hugepage/enabled
echo never > /sys/kernel/mm/transparent_hugepage/defrag
+
Add following lines in /etc/rc.local to disable transparent_hugepage after reboot:
  # Disable transparent hugepages
          if test -f /sys/kernel/mm/transparent_hugepage/enabled; then
            echo never > /sys/kernel/mm/transparent_hugepage/enabled
          fi
          if test -f /sys/kernel/mm/transparent_hugepage/defrag; then
            echo never > /sys/kernel/mm/transparent_hugepage/defrag
          fi

. Disable selinux: change SELINUX=enforcing to SELINUX=disabled. You will need to reboot the host to make change effective.
+
[source, cli]
vi /etc/sysconfig/selinux

. Add following lines to limit.conf to set file descriptor limit and stack size
+
[source, cli]
/etc/security/limits.conf
+
  "*               hard    nofile          65536"
  "*               soft    stack           10240"

. Add swap space to EC2 instance by following this instruction: link:https://aws.amazon.com/premiumsupport/knowledge-center/ec2-memory-swap-file/[How do I allocate memory to work as swap space in an Amazon EC2 instance by using a swap file?^] The exact amount of space to add depends on the size of RAM up to 16G.

. Change node.session.timeo.replacement_timeout in iscsi.conf configuration file to 5 seconds
+
[source, cli]
vi /etc/iscsi/iscsid.conf

. Enable and start iscsi service on the EC2 instance
+
[source, cli]
systemctl enable iscsid
systemctl start iscsid

. Retrieve iscsi initiator address to be used for database luns mapping
+
[source, cli]
cat /etc/iscsi/initiatorname.iscsi

. Add asm group to be used for asm sysasm group
+
[source, cli]
groupadd asm

. Modify oracle user to add asm as a secondary group ( oracle user should have been created after Oracle preinstall rpm installation)
+
[source, cli]
usermod -a -G asm oracle

. Reboot EC2 instance 

====

=== Provision and map database volumes and luns to EC2 instance host
[%collapsible]

====

Provision three volumes from FSx console to host Oracle database binary, data, and logs files.

. Login to FSx cluster via ssh as fsxadmin user

. Execute this command to create a volume for Oracle binary
+ 
[source, cli]
vol create -volume ora_01_biny -aggregate aggr1 -size 50G -state online  -type RW -snapshot-policy none -tiering-policy snapshot-only

. Execute this command to create a volume for Oracle data
+
[source, cli]
vol create -volume ora_01_data -aggregate aggr1 -size 100G -state online  -type RW -snapshot-policy none -tiering-policy snapshot-only

. Execute this command to create a volume for Oracle logs
+ 
[source, cli]
vol create -volume ora_01_logs -aggregate aggr1 -size 100G -state online  -type RW -snapshot-policy none -tiering-policy snapshot-only

. Create binary lun within database binary volume
+
[source, cli]
lun create -path /vol/ora_01_biny/ora_01_biny_01 -size 40G -ostype linux

. Create data luns within database data volume
+
[source, cli]
lun create -path /vol/ora_01_data/ora_01_data_01 -size 20G -ostype linux
lun create -path /vol/ora_01_data/ora_01_data_02 -size 20G -ostype linux
lun create -path /vol/ora_01_data/ora_01_data_03 -size 20G -ostype linux
lun create -path /vol/ora_01_data/ora_01_data_04 -size 20G -ostype linux

. Create logs luns within database logs volume
+
[source, cli]
lun create -path /vol/ora_01_logs/ora_01_logs_01 -size 40G -ostype linux
lun create -path /vol/ora_01_logs/ora_01_logs_02 -size 40G -ostype linux

. Create igroup for EC2 instance with initiator retrieved from step 14 of EC2 kernel configuration above 
+
[source, cli]
igroup create -igroup ora_01 -protocol iscsi -ostype linux -initiator iqn.1994-05.com.redhat:f65fed7641c2

. Map the luns to igroup created above. Increment lun id sequentially for each additional lun within a volume
+
[source, cli]
map -path /vol/ora_01_biny/ora_01_biny_01 -igroup ora_01 -vserver svm_ora -lun-id 0
map -path /vol/ora_01_data/ora_01_data_01 -igroup ora_01 -vserver svm_ora -lun-id 1
map -path /vol/ora_01_data/ora_01_data_02 -igroup ora_01 -vserver svm_ora -lun-id 2
map -path /vol/ora_01_data/ora_01_data_03 -igroup ora_01 -vserver svm_ora -lun-id 3
map -path /vol/ora_01_data/ora_01_data_04 -igroup ora_01 -vserver svm_ora -lun-id 4
map -path /vol/ora_01_logs/ora_01_logs_01 -igroup ora_01 -vserver svm_ora -lun-id 5
map -path /vol/ora_01_logs/ora_01_logs_02 -igroup ora_01 -vserver svm_ora -lun-id 6

. Validate the luns mapping 
+
[source, cli]
mapping show
+
This is expected return:
FsxId02ad7bf3476b741df::> mapping show
  (lun mapping show)
Vserver    Path                                      Igroup   LUN ID  Protocol
---------- ----------------------------------------  -------  ------  --------
svm_ora    /vol/ora_01_biny/ora_01_biny_01           ora_01        0  iscsi
svm_ora    /vol/ora_01_data/ora_01_data_01           ora_01        1  iscsi
svm_ora    /vol/ora_01_data/ora_01_data_02           ora_01        2  iscsi
svm_ora    /vol/ora_01_data/ora_01_data_03           ora_01        3  iscsi
svm_ora    /vol/ora_01_data/ora_01_data_04           ora_01        4  iscsi
svm_ora    /vol/ora_01_logs/ora_01_logs_01           ora_01        5  iscsi
svm_ora    /vol/ora_01_logs/ora_01_logs_02           ora_01        6  iscsi

====

=== Database storage configuration
[%collapsible]

====
Now, import and setup FSx storage for Oracle grid infrastructure and database installation on EC2 instance host.

. Login to EC2 instance via ssh as ec2-user, change to your ssh key and EC2 instance ip address
+
[source, cli]
ssh -i ora_01.pem ec2-user@172.30.15.58

. Discover FSx iSCSI endpoints using either of SVM iSCSI ip addresses, change to your environment specific portal address.
+
[source, cli]
sudo iscsiadm iscsiadm --mode discovery --op update --type sendtargets --portal 172.30.15.51

. Establish iSCSI sessions by logging in to each target
+
[source, cli]
sudo iscsiadm --mode node -l all
+
Expected output from the command:
[ec2-user@ip-172-30-15-58 ~]$ sudo iscsiadm --mode node -l all
Logging in to [iface: default, target: iqn.1992-08.com.netapp:sn.1f795e65c74911edb785affbf0a2b26e:vs.3, portal: 172.30.15.51,3260]
Logging in to [iface: default, target: iqn.1992-08.com.netapp:sn.1f795e65c74911edb785affbf0a2b26e:vs.3, portal: 172.30.15.13,3260]
Login to [iface: default, target: iqn.1992-08.com.netapp:sn.1f795e65c74911edb785affbf0a2b26e:vs.3, portal: 172.30.15.51,3260] successful.
Login to [iface: default, target: iqn.1992-08.com.netapp:sn.1f795e65c74911edb785affbf0a2b26e:vs.3, portal: 172.30.15.13,3260] successful.

. View and validate a list of active iSCSI sessions:
+
[source, cli]
sudo iscsiadm --mode session
+ 
and return the iSCSI sessions
[ec2-user@ip-172-30-15-58 ~]$ sudo iscsiadm --mode session
tcp: [1] 172.30.15.51:3260,1028 iqn.1992-08.com.netapp:sn.1f795e65c74911edb785affbf0a2b26e:vs.3 (non-flash)
tcp: [2] 172.30.15.13:3260,1029 iqn.1992-08.com.netapp:sn.1f795e65c74911edb785affbf0a2b26e:vs.3 (non-flash)

. Validate the luns are imported into the host
+
[source, cli]
sudo sanlun lun show
+
and return a list of Oracle luns from FSx
[ec2-user@ip-172-30-15-58 ~]$ sudo sanlun lun show
controller(7mode/E-Series)/                                   device          host                  lun
vserver(cDOT/FlashRay)        lun-pathname                    filename        adapter    protocol   size    product
+
svm_ora                       /vol/ora_01_logs/ora_01_logs_02 /dev/sdn        host3      iSCSI      40g     cDOT
svm_ora                       /vol/ora_01_logs/ora_01_logs_01 /dev/sdm        host3      iSCSI      40g     cDOT
svm_ora                       /vol/ora_01_data/ora_01_data_03 /dev/sdk        host3      iSCSI      20g     cDOT
svm_ora                       /vol/ora_01_data/ora_01_data_04 /dev/sdl        host3      iSCSI      20g     cDOT
svm_ora                       /vol/ora_01_data/ora_01_data_01 /dev/sdi        host3      iSCSI      20g     cDOT
svm_ora                       /vol/ora_01_data/ora_01_data_02 /dev/sdj        host3      iSCSI      20g     cDOT
svm_ora                       /vol/ora_01_biny/ora_01_biny_01 /dev/sdh        host3      iSCSI      40g     cDOT
svm_ora                       /vol/ora_01_logs/ora_01_logs_02 /dev/sdg        host2      iSCSI      40g     cDOT
svm_ora                       /vol/ora_01_logs/ora_01_logs_01 /dev/sdf        host2      iSCSI      40g     cDOT
svm_ora                       /vol/ora_01_data/ora_01_data_04 /dev/sde        host2      iSCSI      20g     cDOT
svm_ora                       /vol/ora_01_data/ora_01_data_02 /dev/sdc        host2      iSCSI      20g     cDOT
svm_ora                       /vol/ora_01_data/ora_01_data_03 /dev/sdd        host2      iSCSI      20g     cDOT
svm_ora                       /vol/ora_01_data/ora_01_data_01 /dev/sdb        host2      iSCSI      20g     cDOT
svm_ora                       /vol/ora_01_biny/ora_01_biny_01 /dev/sda        host2      iSCSI      40g     cDOT

. Configure multipath.conf file with following default and blacklist entries
+
[source, cli]
sudo vi /etc/multipath.conf
+
defaults {
        find_multipaths yes
        user_friendly_names yes
}
+
blacklist {
        devnode "^(ram|raw|loop|fd|md|dm-|sr|scd|st)[0-9]*"
        devnode "^hd[a-z]"
        devnode "^cciss.*"
}

. Start multipath service
+
[source, cli]
sudo systemctl start multipathd
+ 
now multipath devices appear in /dev/mapper directory
+
[ec2-user@ip-172-30-15-58 ~]$ ls -l /dev/mapper
total 0
lrwxrwxrwx 1 root root       7 Mar 21 20:13 3600a09806c574235472455534e68512d -> ../dm-0
lrwxrwxrwx 1 root root       7 Mar 21 20:13 3600a09806c574235472455534e685141 -> ../dm-1
lrwxrwxrwx 1 root root       7 Mar 21 20:13 3600a09806c574235472455534e685142 -> ../dm-2
lrwxrwxrwx 1 root root       7 Mar 21 20:13 3600a09806c574235472455534e685143 -> ../dm-3
lrwxrwxrwx 1 root root       7 Mar 21 20:13 3600a09806c574235472455534e685144 -> ../dm-4
lrwxrwxrwx 1 root root       7 Mar 21 20:13 3600a09806c574235472455534e685145 -> ../dm-5
lrwxrwxrwx 1 root root       7 Mar 21 20:13 3600a09806c574235472455534e685146 -> ../dm-6
crw------- 1 root root 10, 236 Mar 21 18:19 control

. Login to FSx cluster as fsxadmin user via ssh to retrieve serial-hex number for each lun start with 6c574xxx..., the HEX number start with 3600a0980, which is AWS vendor ID.
+
[source, cli]
lun show -fields serial-hex
+
and return as follow:
+
FsxId02ad7bf3476b741df::> lun show -fields serial-hex
vserver path                            serial-hex
------- ------------------------------- ------------------------
svm_ora /vol/ora_01_biny/ora_01_biny_01 6c574235472455534e68512d
svm_ora /vol/ora_01_data/ora_01_data_01 6c574235472455534e685141
svm_ora /vol/ora_01_data/ora_01_data_02 6c574235472455534e685142
svm_ora /vol/ora_01_data/ora_01_data_03 6c574235472455534e685143
svm_ora /vol/ora_01_data/ora_01_data_04 6c574235472455534e685144
svm_ora /vol/ora_01_logs/ora_01_logs_01 6c574235472455534e685145
svm_ora /vol/ora_01_logs/ora_01_logs_02 6c574235472455534e685146
7 entries were displayed.

. Update /dev/multipath.conf file to add user friendly name for multipath device 
+
[source, cli]
sudo vi /etc/multipath.conf
+
with following entries:
multipaths {
        multipath {
                wwid            3600a09806c574235472455534e68512d
                alias           ora_01_biny_01
        }
        multipath {
                wwid            3600a09806c574235472455534e685141
                alias           ora_01_data_01
        }
        multipath {
                wwid            3600a09806c574235472455534e685142
                alias           ora_01_data_02
        }
        multipath {
                wwid            3600a09806c574235472455534e685143
                alias           ora_01_data_03
        }
        multipath {
                wwid            3600a09806c574235472455534e685144
                alias           ora_01_data_04
        }
        multipath {
                wwid            3600a09806c574235472455534e685145
                alias           ora_01_logs_01
        }
        multipath {
                wwid            3600a09806c574235472455534e685146
                alias           ora_01_logs_02
        }

. Reboot multipath service to valiate the devices under /dev/mapper have changed to lun names as versus serial-hex ids.
+
[source, cli]
sudo systemctl restart multipathd
+
check /dev/mapper to return as following:
+
[ec2-user@ip-172-30-15-58 ~]$ ls -l /dev/mapper
total 0
crw------- 1 root root 10, 236 Mar 21 18:19 control
lrwxrwxrwx 1 root root       7 Mar 21 20:41 ora_01_biny_01 -> ../dm-0
lrwxrwxrwx 1 root root       7 Mar 21 20:41 ora_01_data_01 -> ../dm-1
lrwxrwxrwx 1 root root       7 Mar 21 20:41 ora_01_data_02 -> ../dm-2
lrwxrwxrwx 1 root root       7 Mar 21 20:41 ora_01_data_03 -> ../dm-3
lrwxrwxrwx 1 root root       7 Mar 21 20:41 ora_01_data_04 -> ../dm-4
lrwxrwxrwx 1 root root       7 Mar 21 20:41 ora_01_logs_01 -> ../dm-5
lrwxrwxrwx 1 root root       7 Mar 21 20:41 ora_01_logs_02 -> ../dm-6

. Partition the binary lun with single primary partition
+
[source, cli]
sudo fdisk /dev/mapper/ora_01_biny_01

. Format partitioned binary lun with XFS file system
+
[source, cli]
sudo mkfs.xfs /dev/mapper/ora_01_biny_01p1

. Mount the binary lun to /u01
+
[source, cli]
sudo mount -t xfs /dev/mapper/ora_01_biny_01p1 /u01

. Change /u01 mount point ownership to oracle user and it's asssociated primary group
+
[source, cli]
chown oracle:oinstall /u01

. Add mount point to /etc/fstab 
+
[source, cli]
sudo vi /etc/fstab
+
add this line "/dev/mapper/ora_01_biny_01p1 /u01 xfs defaults 0 0"

. Download and staging Oracle binary installation files to /tmp/archive directory
+
The list of installation files to be stated in /tmp/archive on EC2 instance
+
[ec2-user@ip-172-30-15-58 ~]$ ls -l /tmp/archive
total 10537316
-rw-rw-r--. 1 ec2-user ec2-user      19112 Mar 21 15:57 compat-libcap1-1.10-7.el7.x86_64.rpm
-rw-rw-r--  1 ec2-user ec2-user 3059705302 Mar 21 22:01 LINUX.X64_193000_db_home.zip
-rw-rw-r--  1 ec2-user ec2-user 2889184573 Mar 21 21:09 LINUX.X64_193000_grid_home.zip
-rw-rw-r--. 1 ec2-user ec2-user     589145 Mar 21 15:56 netapp_linux_unified_host_utilities-7-1.x86_64.rpm
-rw-rw-r--. 1 ec2-user ec2-user      31828 Mar 21 15:55 oracle-database-preinstall-19c-1.0-2.el8.x86_64.rpm
-rw-rw-r--  1 ec2-user ec2-user 2872741741 Mar 21 22:31 p34762026_190000_Linux-x86-64.zip
-rw-rw-r--  1 ec2-user ec2-user 1843577895 Mar 21 22:32 p34765931_190000_Linux-x86-64.zip
-rw-rw-r--  1 ec2-user ec2-user  124347218 Mar 21 22:33 p6880880_190000_Linux-x86-64.zip
-rw-r--r--  1 ec2-user ec2-user     257136 Mar 22 16:25 policycoreutils-python-utils-2.9-9.el8.noarch.rpm
====

=== Oracle grid infrastructure installation
[%collapsible]

====
. Login to EC2 instance as ec2-user via ssh and enable password authentication by uncomment "PasswordAuthentication yes" and comment out "PasswordAuthentication no" 
+
[source, cli]
sudo vi /etc/ssh/sshd_config

. Restart sshd service
+
[source, cli]
sudo systemctl restart sshd

. Reset oracle user password
+
[source, cli]
sudo passwd oracle

. Login as the Oracle Restart software owner user (oracle). Create Oracle directory as follow:
+
[source, cli]
mkdir -p /u01/app/oracle
mkdir -p /u01/app/oraInventory

. Change directory permission setting
+
[source, cli]
chmod -R 775 /u01/app

. Create grid home directory and change to grid home directory
+
[source, cli]
mkdir -p /u01/app/oracle/product/19.0.0/grid
cd /u01/app/oracle/product/19.0.0/grid

. Unzip grid installation files
+
[source, cli]
unzip -q /tmp/archive/LINUX.X64_193000_grid_home.zip

. From grid home, delete OPatch directory
+
[source, cl]
rm -rf OPatch

. From grid home, copy p6880880_190000_Linux-x86-64.zip to grid_home, then unzip it
+
[source, cli]
cp /tmp/archive/p6880880_190000_Linux-x86-64.zip .
unzip p6880880_190000_Linux-x86-64.zip

. From grid home, revise cv/admin/cvu_config, uncomment and replace "CV_ASSUME_DISTID=OEL5" to "CV_ASSUME_DISTID=OL7"
+
[source, cli]
vi cv/admin/cvu_config

. Prepare a gridsetup.rsp file for silent installation and place the rsp file in /tmp/archive directory. The rsp file should cover section A,B and G with following infomation:
+
INVENTORY_LOCATION=/u01/app/oraInventory
oracle.install.option=HA_CONFIG
ORACLE_BASE=/u01/app/oracle
oracle.install.asm.OSDBA=dba
oracle.install.asm.OSOPER=oper
oracle.install.asm.OSASM=asm
oracle.install.asm.SYSASMPassword="SetPWD"
oracle.install.asm.diskGroup.name=DATA
oracle.install.asm.diskGroup.redundancy=EXTERNAL
oracle.install.asm.diskGroup.AUSize=4
oracle.install.asm.diskGroup.disks=/dev/mapper/ora_01_data*
oracle.install.asm.diskGroup.diskDiscoveryString=/dev/mapper/*
oracle.install.asm.monitorPassword="SetPWD"
oracle.install.asm.configureAFD=true

. Login in to EC2 instance as root user and set ORACLE_HOME and ORACLE_BASE
+
[source, cli]
export ORACLE_HOME=/u01/app/oracle/product/19.0.0/grid
export ORACLE_BASE=/tmp
cd /u01/app/oracle/product/19.0.0/grid/bin

. Provision disk devices for use with Oracle ASM filter driver
+
[source, cli]
./asmcmd afd_label DATA01 /dev/mapper/ora_01_data_01 --init
+
[source, cli]
./asmcmd afd_label DATA02 /dev/mapper/ora_01_data_02 --init
+
[source, cli]
./asmcmd afd_label DATA03 /dev/mapper/ora_01_data_03 --init
+
[source, cli]
./asmcmd afd_label DATA04 /dev/mapper/ora_01_data_04 --init
+
[source, cli]
./asmcmd afd_label LOGS01 /dev/mapper/ora_01_logs_01 --init
+
[source, cli]
./asmcmd afd_label LOGS02 /dev/mapper/ora_01_logs_02 --init

. Change devices ownership to oracle:oinstall
+
[source, cli]
chown oracle:oinstall /dev/mapper/ora_01_data_01
chown oracle:oinstall /dev/mapper/ora_01_data_02
chown oracle:oinstall /dev/mapper/ora_01_data_03
chown oracle:oinstall /dev/mapper/ora_01_data_04
chown oracle:oinstall /dev/mapper/ora_01_logs_01
chown oracle:oinstall /dev/mapper/ora_01_logs_02

. Install cvuqdisk-1.0.10-1.rpm
+
[source, cli]
rpm -ivh /u01/app/oracle/product/19.0.0/grid/cv/rpm/cvuqdisk-1.0.10-1.rpm

. Install policycoreutils-python-utils not available in EC2 instance
+
[source, cli]
yum install /tmp/archive/policycoreutils-python-utils-2.9-9.el8.noarch.rpm

. Unset $ORACLE_BASE
+
[source, cli]
unset ORACLE_BASE

. Login EC2 instance as oracle user and extract patch in /tmp/archive folder 
+
[source, cli]
unzip p34762026_190000_Linux-x86-64.zip

. As oracle user launch gridSetup.sh for grid infrastructure installation
+
[source, cli]
./gridSetup.sh -applyRU /tmp/archive/34762026/ -silent -responseFile /tmp/archive/gridsetup.rsp

. As a root user, execute the following script(s)
+
[source, cli]
/u01/app/oraInventory/orainstRoot.sh
+
[source, cli]
/u01/app/oracle/product/19.0.0/grid/root.sh

. As oracle user, execute the following command to complete the configuration.
+
[source, cli]
/u01/app/oracle/product/19.0.0/grid/gridSetup.sh -executeConfigTools -responseFile /tmp/archive/gridsetup.rsp -silent

. As oracle user, create LOGS disk group
+
[source, cli]
bin/asmca -silent -sysAsmPassword 'yourPWD' -asmsnmpPassword 'yourPWD' -createDiskGroup -diskString '/dev/mapper/*' -diskGroupName LOGS -disklist '/dev/mapper/ora_logs_01,/dev/mapper/ora_logs_02' -redundancy EXTERNAL

. If needed, as oracle user, create DATA disk group if not created through grid installation
+
[source, cli]
bin/asmca -silent -sysAsmPassword 'yourPWD' -asmsnmpPassword 'yourPWD' -createDiskGroup -diskString '/dev/mapper/*' -diskGroupName DATA -disklist '/dev/mapper/ora_01_data_01,/dev/mapper/ora_01_data_02,/dev/mapper/ora_01_data_03,/dev/mapper/ora_01_data_04' -redundancy EXTERNAL 

. As oracle user, validate grid services after installation configuration
+
[source, cli]
bin/crsctl stat res -t
+
Name                Target  State        Server                   State details
Local Resources
ora.DATA.dg         ONLINE  ONLINE       ip-172-30-15-58          STABLE
ora.LISTENER.lsnr   ONLINE  ONLINE       ip-172-30-15-58          STABLE
ora.LOGS.dg         ONLINE  ONLINE       ip-172-30-15-58          STABLE
ora.asm             ONLINE  ONLINE       ip-172-30-15-58          Started,STABLE
ora.ons             OFFLINE OFFLINE      ip-172-30-15-58          STABLE
Cluster Resources
ora.cssd            ONLINE  ONLINE       ip-172-30-15-58          STABLE
ora.diskmon         OFFLINE OFFLINE                               STABLE
ora.driver.afd      ONLINE  ONLINE       ip-172-30-15-58          STABLE
ora.evmd            ONLINE  ONLINE       ip-172-30-15-58          STABLE

. Valiate ASM filter driver status
+
[oracle@ip-172-30-15-58 grid]$ asmcmd
ASMCMD> lsdg
State    Type    Rebal  Sector  Logical_Sector  Block       AU  Total_MB  Free_MB  Req_mir_free_MB  Usable_file_MB  Offline_disks  Voting_files  Name
MOUNTED  EXTERN  N         512             512   4096  1048576     81920    81847                0           81847              0             N  DATA/
MOUNTED  EXTERN  N         512             512   4096  1048576     81920    81853                0           81853              0             N  LOGS/
ASMCMD> afd_state
ASMCMD-9526: The AFD state is 'LOADED' and filtering is 'ENABLED' on host 'ip-172-30-15-58.ec2.internal'

====

=== Oracle database installation
[%collapsible]


=== Automated Deployment Option
[%collapsible]

NetApp will release a fully automated solution deployment toolkit with Ansible to facilitate the implementation of the solution. Please check back for the availability of the toolkit. Once it is released, a link will be posted here.

== Oracle Database Backup, Restore, and Clone with SnapCenter Service

== Additional Information

To learn more about the information that is described in this document, review the following documents and/or websites:

* Installing Oracle Grid Infrastructure for a Standalone Server with a New Database Installation 
+
link:https://docs.oracle.com/en/database/oracle/oracle-database/19/ladbi/installing-oracle-grid-infrastructure-for-a-standalone-server-with-a-new-database-installation.html#GUID-0B1CEE8C-C893-46AA-8A6A-7B5FAAEC72B3[https://docs.oracle.com/en/database/oracle/oracle-database/19/ladbi/installing-oracle-grid-infrastructure-for-a-standalone-server-with-a-new-database-installation.html#GUID-0B1CEE8C-C893-46AA-8A6A-7B5FAAEC72B3^]

*  Installing and Configuring Oracle Database Using Response Files
+
link:https://docs.oracle.com/en/database/oracle/oracle-database/19/ladbi/installing-and-configuring-oracle-database-using-response-files.html#GUID-D53355E9-E901-4224-9A2A-B882070EDDF7[https://docs.oracle.com/en/database/oracle/oracle-database/19/ladbi/installing-and-configuring-oracle-database-using-response-files.html#GUID-D53355E9-E901-4224-9A2A-B882070EDDF7^]


* Amazon FSx for NetApp ONTAP
+
link:https://aws.amazon.com/fsx/netapp-ontap/[https://aws.amazon.com/fsx/netapp-ontap/^]

* Amazon EC2
+
link:https://aws.amazon.com/pm/ec2/?trk=36c6da98-7b20-48fa-8225-4784bced9843&sc_channel=ps&s_kwcid=AL!4422!3!467723097970!e!!g!!aws%20ec2&ef_id=Cj0KCQiA54KfBhCKARIsAJzSrdqwQrghn6I71jiWzSeaT9Uh1-vY-VfhJixF-xnv5rWwn2S7RqZOTQ0aAh7eEALw_wcB:G:s&s_kwcid=AL!4422!3!467723097970!e!!g!!aws%20ec2[https://aws.amazon.com/pm/ec2/?trk=36c6da98-7b20-48fa-8225-4784bced9843&sc_channel=ps&s_kwcid=AL!4422!3!467723097970!e!!g!!aws%20ec2&ef_id=Cj0KCQiA54KfBhCKARIsAJzSrdqwQrghn6I71jiWzSeaT9Uh1-vY-VfhJixF-xnv5rWwn2S7RqZOTQ0aAh7eEALw_wcB:G:s&s_kwcid=AL!4422!3!467723097970!e!!g!!aws%20ec2^]

