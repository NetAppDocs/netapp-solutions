---
sidebar: sidebar
permalink: databases/aws_ora_fsx_ec2_iscsi_asm.html
keywords: Oracle, AWS, FSx ONTAP, Database, Oracle ASM, Oracle Restart, iSCSI
summary: "The solution provides overview and details for Oracle database deployment and protection in AWS FSx ONTAP storage and EC2 compute instance with iSCSI protocol and Oracle database configured in standalone ReStart using asm as volume manager." 
---

= TR-XXXX: Oracle Database Deployment and Protection in AWS FSx/EC2 with iSCSI/ASM
:hardbreaks:
:nofooter:
:icons: font
:linkattrs:
:imagesdir: ./../media/

Allen Cao, Niyaz Mohamed, NetApp

[.lead]

== Purpose

ASM - automatic storage management is a popular Oracle storage volume manager that is employed in many Oracle installations. Since Oracle version 11g, ASM has been packaged with grid infrastructure rather than database. As a result, in order to utilize Oracle ASM for storage management without RAC, Oracle grid infrastructure can be installed in a Standalone server, also known as Oracle Restart. It certainly adds more complexity in an otherwise simpler Oracle database deployment. However, as the name implies, when Oracle is deployed in Restart mode, it gains the capability to restart all failed Oracle services without user intervention. It achieves certain degree of high availability or HA functionality.

In this documentation, we demonstrate how you can deploy Oracle database with iSCSI protocol and Oracle ASM in AWS FSx ONTAP storage and EC2 compute instances environment step by step. We will also demonstrate how you could use NetApp SnapCenter service through NetApp BlueXP console to backup, restore, and clone your Oracle database for DEV/TEST or any other use cases for storage efficient database operation in AWS public cloud.  

This solution addresses the following use cases:

* Oracle database deployment in AWS FSx ONTAP storage and EC2 compute instances with iSCSI/ASM 
* Testing and validating a Oracle workload in the public AWS cloud with iSCSI/ASM
* Testing and validating a Oracle database Restart functionalities deployed in AWS

== Audience

This solution is intended for the following people:

* The DBA who is interested in deploying Oracle in AWS public cloud with iSCSI/ASM.
* The database solution architect who is interested in testing Oracle workloads in the public AWS cloud.
* The storage administrator who is interested in deploying and managing Oracle database deployed to AWS FSx storage.
* The application owner who is interested in standing up an Oracle database in AWS FSx/EC2.

== Solution test and validation environment

The testing and validation of this solution was performed in an AWS FSx and EC2 environment that might not match the final deployment environment. For more information, see the section <<Key Factors for Deployment Consideration>>.

=== Architecture

image::aws_ora_fsx_ec2_iscsi_asm_architecture.png["This image provides a detailed picture of the Oracle deployment configuration in AWS public cloud with iSCSI and ASM."]

=== Hardware and software components

[%autowidth.stretch]
|===
3+^| *Hardware*
| FSx ONTAP storage | Current version offered by AWS | One FSx HA cluster in the same VPC and availability zone
| EC2 instance for compute | t2.xlarge/4vCPU/16G | Two EC2 T2 xlarge EC2 instances, one as primary DB server and the other as clone DB server 

3+^| *Software*
| RedHat Linux | RHEL-8.6.0_HVM-20220503-x86_64-2-Hourly2-GP2 | Deployed RedHat subscription for testing
| Oracle Grid Infrastructure | Version 19.18 | Applied RU patch on top of 19.3 base release p34762026_190000_Linux-x86-64.zip
| Oracle Database | Version 19.18 | | Applied RU patch on top of 19.3 base release p34765931_190000_Linux-x86-64.zip
| Oracle OPatch | Version 12.2.0.1.36 | Required for version 18 of RU patch p6880880_190000_Linux-x86-64.zip
| SnapCenter Service | Version |  
|===

=== Key factors for deployment consideration

* *EC2 compute instances.* In these tests and validations, we used the AWS EC2 t2.xlarge instance type for the Oracle database compute instance. NetApp recommends using an M5 type EC2 instance as the compute instance for Oracle in production deployment because it is optimized for database workloads. You need to size the EC2 instance appropriately in terms of number of vCPU and RAM based on actual workload requirements.

* *FSx storage HA clusters single- or multi-zone deployment.* In these tests and validations, we deployed an FSx HA cluster in a single AWS availability zone. For production deployment, NetApp recommends deploying an FSx HA pair in two different availability zones. An FSx HA cluster is alway provisioned in a HA pair that is sync mirrored in a pair of active-passive file systems to provide storage-level redundancy. Multi zones deployment further enhances high availability in the event of failure in a single AWS zone. 

* *FSx storage cluster sizing.* A FSx ONTAP storage file system provides up to 160,000 raw SSD IOPS, up to 4GB/s throughput, and maximum 192 TiB capacity. However, you can size the cluster in terms of provisioned IOPS, throughput, and storage limit (minimum 1,024 GiB) totally based on your actually requirements at the time of deployment. The capacity can be adjusted dynamically on the fly without any impact to application availability.   

* *Oracle data and logs layout.* In our tests and validations, we have deployed two ASM disk groups for data and logs respectively. Within +DATA asm disk group, we provisioned 4 luns in a data volume. Within +LOGS asm disk group, we provisioned two luns in a logs volume. In general, multiple luns layout within a FSx ONTAP volume provides better performance. 

* *iSCSI configuration.* The EC2 instance DB server connects to FSx storage via iSCSI protocol. EC2 instances generally deploy with a single network interface or ENI. The single NIC interface will carry both iSCSI and application traffic. It is important to gauge Oracle database peek IO throughput requirement by carefully analyzing Oracle AWR report in order to choose a right EC2 compute instance that meets both application and iSCSI traffic throughput requirement. NetApp also recommends to allocate 4 iSCSI connections to both FSx iSCSI endpoints with multipath properly configured.       

* *Database backup.* NetApp provides a SaaS version of SnapCenter software service for database backup, restore, and clone in cloud that is available via NetApp BlueXP console UI. It is recommended to implement such service to achieve fast (under a minute) SnapShot backup, quick (few minutes) database restore, and database clone.    

== Solution Deployment

The following provides the outline and details of step by step deployment procedures. 

=== Prerequisites for deployment
[%collapsible]
====

Deployment requires the following prerequisites.

. An AWS account has been set up, and the necessary VPC and network segments have been created within your AWS account.

. From the AWS EC2 console, you need to deploy two EC2 Linux instances, one as the primary Oracle DB server and an optional alternative clone target DB server. See the architecture diagram in the previous section for more details about the environment setup. Also review the link:https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/concepts.html[User Guide for Linux instances] for more information.

. From the AWS EC2 console, deploy a FSx ONTAP storage HA clusters to host the Oracle database volumes. If you are not familiar with the deployment of FSx storage, see the documentation link:https://docs.aws.amazon.com/fsx/latest/ONTAPGuide/creating-file-systems.html[Creating FSx for ONTAP file systems] for step-by-step instructions.

. The above step 2 and 3 can be setup using following Terraform automation toolkit, which will create an EC2 instance named ora_01 and a FSx file system named as fsx_01. Review the instruction carefully and change the variables to your environment specifics before execution.
+
[source, cli]
git clone https://github.com/NetApp-Automation/na_aws_fsx_ec2_deploy.git

====

=== EC2 instance kernel configuration
[%collapsible]
====
With prerequisites provisioned, login into EC2 instance to configure linux kernel for Oracle installation.

. From Oracle, download and install Oracle 19c preinstall RPM, which will satisfy most kernel configuration requirement
+
[source, cli]
yum install oracle-database-preinstall-19c-1.0-2.el8.x86_64.rpm

. Download and install missing compat-libcap1 in Linux 8
+
[source, cli]
yum install compat-libcap1-1.10-7.el7.x86_64.rpm

. From NetApp, download and install NetApp host utilities 
+
[source, cli]
yum install netapp_linux_unified_host_utilities-7-1.x86_64.rpm

. Install open JDK version 1.8
+
[source, cli]
yum install java-1.8.0-openjdk.x86_64

. Install iSCSI initiator utils
+
[source, cli]
yum install iscsi-initiator-utils

. Install sg3_utils
+
[source, cli]
yum install sg3_utils

. Install device-mapper-multipath
+
[source, cli]
yum install device-mapper-multipath

. Disable transparent hugepages in current system
+
[source, cli]
echo never > /sys/kernel/mm/transparent_hugepage/enabled
echo never > /sys/kernel/mm/transparent_hugepage/defrag
+
Add following lines in /etc/rc.local to disable transparent_hugepage after reboot:
  # Disable transparent hugepages
          if test -f /sys/kernel/mm/transparent_hugepage/enabled; then
            echo never > /sys/kernel/mm/transparent_hugepage/enabled
          fi
          if test -f /sys/kernel/mm/transparent_hugepage/defrag; then
            echo never > /sys/kernel/mm/transparent_hugepage/defrag
          fi

. Disable selinux: change SELINUX=enforcing to SELINUX=disabled. You will need to reboot the host to make change effective.
+
[source, cli]
vi /etc/sysconfig/selinux

. Add following lines to limit.conf to set file descriptor limit and stack size
+
[source, cli]
/etc/security/limits.conf
+
  "*               hard    nofile          65536"
  "*               soft    stack           10240"

. Add swap space to EC2 instance by following this instruction: https://https://aws.amazon.com/premiumsupport/knowledge-center/ec2-memory-swap-file/[How do I allocate memory to work as swap space in an Amazon EC2 instance by using a swap file?^] The exact amount of space to add depends on the size of RAM up to 16G.

. Change node.session.timeo.replacement_timeout in iscsi.conf configuration file to 5 seconds
+
[source, cli]
vi /etc/iscsi/iscsid.conf

. Enable and start iscsi service on the EC2 instance
+
[source, cli]
systemctl enable iscsid
systemctl start iscsid

. Retrieve iscsi initiator address to be used for database luns mapping
+
[source, cli]
cat /etc/iscsi/initiatorname.iscsi

. Reboot EC2 instance 


====

=== Provision database volumes and luns

=== Database storage configuration

=== Oracle grid infrastructure installation

=== Oracle database installation


=== Configure the hosts file

Input the primary FSx ONTAP cluster management IP and EC2 instances hosts names into the hosts file.

  # Primary FSx cluster management IP address
  [fsx_ontap]  
  172.30.15.33

  # Primary PostgreSQL DB server at primary site where database is initialized at deployment time
  [postgresql] 
  psql_01p ansible_ssh_private_key_file=psql_01p.pem

  # Primary PostgreSQL DB server at standby site where postgresql service is installed but disabled at deployment
  # Standby DB server at primary site, to setup this server comment out other servers in [dr_postgresql]
  # Standby DB server at standby site, to setup this server comment out other servers in [dr_postgresql]
  [dr_postgresql] --   
  psql_01s ansible_ssh_private_key_file=psql_01s.pem
  #psql_01ps ansible_ssh_private_key_file=psql_01ps.pem  
  #psql_01ss ansible_ssh_private_key_file=psql_01ss.pem

=== Configure the host_name.yml file in the host_vars folder

include::../_include/aws_postgres_fsx_ec2_host_vars.adoc[]

=== Configure the global fsx_vars.yml file in the vars folder

include::../_include/aws_postgres_fsx_ec2_fsx_vars.adoc[]

=== PostgreSQL deployment and HA/DR setup

The following tasks deploy the PostgreSQL DB server service and initialize the database at the primary site on the primary EC2 DB server host. A standby primary EC2 DB server host is then set up at the standby site. Finally, DB volume replication is set up from the primary-site FSx cluster to the standby-site FSx cluster for disaster recovery.  

. Create DB volumes on the primary FSx cluster, and set up postgresql on the primary EC2 instance host.
[source, cli]
ansible-playbook -i hosts postgresql_deploy.yml -u ec2-user --private-key psql_01p.pem -e @vars/fsx_vars.yml

. Set up the standby DR EC2 instance host.
[source, cli]
ansible-playbook -i hosts postgresql_standby_setup.yml -u ec2-user --private-key psql_01s.pem -e @vars/fsx_vars.yml

. Set up FSx ONTAP cluster peering and database volume replication.
[source, cli]
ansible-playbook -i hosts fsx_replication_setup.yml -e @vars/fsx_vars.yml

. Consolidate the previous steps into a single-step PostgreSQL deployment and HA/DR setup. 
[source, cli]
ansible-playbook -i hosts postgresql_hadr_setup.yml -u ec2-user -e @vars/fsx_vars.yml

. For setting up a standby PostgreSQL DB host at either the primary or standby sites, comment out all other servers in the hosts file [dr_postgresql] section and then execute the postgresql_standby_setup.yml playbook with the respective target host (such as psql_01ps or standby EC2 compute instance at primary site). Make sure that a host parameters file such as `psql_01ps.yml` is configured under the `host_vars` directory. 
[source, cli]
[dr_postgresql] --   
#psql_01s ansible_ssh_private_key_file=psql_01s.pem
psql_01ps ansible_ssh_private_key_file=psql_01ps.pem  
#psql_01ss ansible_ssh_private_key_file=psql_01ss.pem
+
[source, cli]
ansible-playbook -i hosts postgresql_standby_setup.yml -u ec2-user --private-key psql_01ps.pem -e @vars/fsx_vars.yml

=== PostgreSQL database snapshot backup and replication to standby site

PostgreSQL database snapshot backup and replication to the standby site can be controlled and executed on the Ansible controller with a user-defined interval. We have validated that the interval can be as low as 5 minutes. Therefore, in the case of failure at the primary site, there is 5 minutes of potential data loss if failure occurs right before the next scheduled snapshot backup.

[source, cli]
*/15 * * * * /home/admin/na_postgresql_aws_deploy_hadr/data_log_snap.sh

=== Failover to Standby Site for DR

For testing the PostgreSQL HA/DR system as a DR exercise, execute failover and PostgreSQL database recovery on the primary standby EC2 DB instance on standby site by executing following playbook. In an actually DR scenario, execute the same for an actually failover to DR site.  

[source, cli]
ansible-playbook -i hosts postgresql_failover.yml -u ec2-user --private-key psql_01s.pem -e @vars/fsx_vars.yml 

=== Resync Replicated DB volumes after Failover Test

Run resync after the failover test to reestablish database-volume SnapMirror replication.

[source, cli]
ansible-playbook -i hosts postgresql_standby_resync.yml -u ec2-user --private-key psql_01s.pem -e @vars/fsx_vars.yml

=== Failover from primary EC2 DB server to standby EC2 DB server due to EC2 compute instance failure 

NetApp recommends running manual failover or using well-established OS cluster-ware that might require a license.

== Automated Deployment Option

NetApp will release a fully automated solution deployment toolkit with Ansible to facilitate the implementation of the solution. Please check back for the availability of the toolkit. Once it is released, a link will be posted here.

== Oracle Database Backup, Restore, and Clone with SnapCenter Service

== Additional Information

To learn more about the information that is described in this document, review the following documents and/or websites:

* Amazon FSx for NetApp ONTAP
+
link:https://aws.amazon.com/fsx/netapp-ontap/[https://aws.amazon.com/fsx/netapp-ontap/^]

* Amazon EC2
+
link:https://aws.amazon.com/pm/ec2/?trk=36c6da98-7b20-48fa-8225-4784bced9843&sc_channel=ps&s_kwcid=AL!4422!3!467723097970!e!!g!!aws%20ec2&ef_id=Cj0KCQiA54KfBhCKARIsAJzSrdqwQrghn6I71jiWzSeaT9Uh1-vY-VfhJixF-xnv5rWwn2S7RqZOTQ0aAh7eEALw_wcB:G:s&s_kwcid=AL!4422!3!467723097970!e!!g!!aws%20ec2[https://aws.amazon.com/pm/ec2/?trk=36c6da98-7b20-48fa-8225-4784bced9843&sc_channel=ps&s_kwcid=AL!4422!3!467723097970!e!!g!!aws%20ec2&ef_id=Cj0KCQiA54KfBhCKARIsAJzSrdqwQrghn6I71jiWzSeaT9Uh1-vY-VfhJixF-xnv5rWwn2S7RqZOTQ0aAh7eEALw_wcB:G:s&s_kwcid=AL!4422!3!467723097970!e!!g!!aws%20ec2^]

* NetApp Solution Automation
+
link:https://docs.netapp.com/us-en/netapp-solutions/automation/automation_introduction.html[https://docs.netapp.com/us-en/netapp-solutions/automation/automation_introduction.html^]
