---
sidebar: sidebar
permalink: virtualization/vmware_vcf_asa_supp_wkld_nvme.html
keywords: netapp, vmware, cloud, foundation, vcf, aff, all-flash, nfs, vvol, vvols, array, ontap tools, otv, sddc, iscsi
summary:
---

:hardbreaks:
:nofooter:
:icons: font
:linkattrs:
:imagesdir: ./../media/

[.lead]
Author: Josh Powell

== Configure NVMe/TCP supplemental storage for VCF Workload Domains

=== Scenario Overview
In this scenario we will demonstrate how to configure NVMe/TCP supplemental storage for a VCF Workload Domain. 

This scenario covers the following high level steps:

* Create a storage virtual machine (SVM) with logical interfaces (LIFs) for iSCSI traffic.
* Create distributed port groups for iSCSI networks on the VI Workload Domain.
* Create vmkernel adapters for iSCSI on the ESXi hosts for the VI Workload Domain.
* 

=== Prerequisites
This scenario requires the following components and configurations:

* An ONTAP ASA storage system with physical data ports on ethernet switches dedicated to storage traffic.
* VCF Management Domain deployment is complete and the vSphere client is accessible.
* A VI Workload Domain has been previously deployed.

NetApp recommends fully redundant network designs for NVMe/TCP. The following diagram illustrates an example of a redundant configuration, providing fault tolerance for storage systems, switches, networks adapters and host systems. Refer to the NetApp link:https://docs.netapp.com/us-en/ontap/san-config/index.html[SAN configuration reference] for additional information.

image:vmware-vcf-asa-image74.png[NVMe-tcp network design]

For multipathing and failover across multiple paths, NetApp recommends having a minimum of two LIFs per storage node in separate ethernet networks for all SVMs in NVMe/TCP configurations.

This documentation demonstrates the process of creating a new SVM and specifying the IP address information to create multiple LIFs for NVMe/TCP traffic. To add new LIFs to an existing SVM refer to link:https://docs.netapp.com/us-en/ontap/networking/create_a_lif.htm[Create a LIF (network interface)].

For additional information on NVMe design considerations for ONTAP storage systems, refer to link:https://docs.netapp.com/us-en/ontap/nvme/support-limitations.html[NVMe configuration, support and limitations]. 

=== Deployment Steps
To create a VMFS datastore on a VCF Workload Domain using NVMe/TCP, complete the following steps.

==== Create SVM, LIFs and NVMe Namespace on ONTAP storage system
The following step is performed in ONTAP System Manager.

.Create the storage VM and LIFs
[%collapsible]
==== 
Complete the following steps to create an SVM together with multiple LIFs for NVMe/TCP traffic.

. From ONTAP System Manager navigate to *Storage VMs* in the left-hand menu and click on *+ Add* to start. 
+
image:vmware-vcf-asa-image01.png[Click +Add to start creating SVM]
+
{nbsp}
. In the *Add Storage VM* wizard provide a *Name* for the SVM, select the *IP Space* and then, under *Access Protocol*, click on the *NVMe* tab and check the box to *Enable NVMe/TCP*.
+
image:vmware-vcf-asa-image75.png[Add storage VM wizard - enable NVMe/TCP]
+
{nbsp}
. In the *Network Interface* section fill in the *IP address*, *Subnet Mask*, and *Broadcast Domain and Port* for the first LIF. For subsequent LIFs the checkbox may be enabled to use common settings across all remaining LIFs, or use separate settings.
+
NOTE: For multipathing and failover across multiple paths, NetApp recommends having a minimum of two LIFs per storage node in separate Ethernet networks for all SVMs in NVMe/TCP configurations.
+
image:vmware-vcf-asa-image76.png[Fill out network info for LIFs]
+
{nbsp}
. Choose whether to enable the Storage VM Administration account (for multi-tenancy environments) and click on *Save* to create the SVM.
+
image:vmware-vcf-asa-image04.png[Enable SVM account and Finish]
====

.Create the NVMe Namespace
[%collapsible]
==== 
NVMe namespaces are analogous to LUNs for iSCSi or FC. The NVMe Namespace must be created before a VMFS datastore can be deployed from the vSphere Client. To create the NVMe namespace, the NVMe Qualified Name (NQN) must first be obtained from each ESXi host in the cluster. The NQN is used by ONTAP to provide access control for the namespace. 

Complete the following steps to create an NVMe Namespace:

. Open an SSH session with an ESXi host in the cluster to obtain its NQN. Use the following command from the CLI:
+
[source, cli]
esxcli nvme info get
+
An output similar to the following should be displayed:
+
[source, cli]
Host NQN: nqn.2014-08.com.netapp.sddc:nvme:vcf-wkld-esx01

. Record the NQN for each ESXi host in the cluster

. From ONTAP System Manager navigate to *NVMe Namespaces* in the left-hand menu and click on *+ Add* to start. 
+
image:vmware-vcf-asa-image93.png[Click +Add to create NVMe Namespace]
+
{nbsp}
. On the *Add NVMe Namespace* page, fill in a name prefix, the number of namespaces to create, the size of the namespace, and the host operating system that will be accessing the namespace. In the *Host NQN* section create a comma separated list of the NQN's previously collected from the ESXi hosts that will be accessing the namespaces. 

Click on *More Options* to configure additional items such as the snapshot protection policy. Finally, click on *Save* to create the NVMe Namespace.
+
image:vmware-vcf-asa-image93.png[Click +Add to create NVMe Namespace]
====

==== Set up networking and NVMe software adapters on ESXi hosts
The following steps are performed on the VI Workload Domain cluster using the vSphere client. In this case vCenter Single Sign-On is being used so the vSphere client is common to both the management and workload domains.

.Create Distributed Port Groups for NVME/TCP traffic
[%collapsible]
====
Complete the following to create a new distributed port group for each NVMe/TCP network:

. From the vSphere client , navigate to *Inventory > Networking* for the Workload Domain. Navigate to the existing Distributed Switch and choose the action to create *New Distributed Port Group...*.
+
image:vmware-vcf-asa-image22.png[Choose to create new port group]
+
{nbsp}
. In the *New Distributed Port Group* wizard fill in a name for the new port group and click on *Next* to continue.

. On the *Configure settings* page fill out all settings. If VLANs are being used be sure to provide the correct VLAN ID. Click on *Next* to continue.
+
image:vmware-vcf-asa-image23.png[Fill out VLAN ID]
+
{nbsp}
. On the *Ready to complete* page, review the changes and click on *Finish* to create the new distributed port group.

. Repeat this process to create a distributed port group for the second NVMe/TCP network being used and ensure you have input the correct *VLAN ID*.

. Once both port groups have been created, navigate to the first port group and select the action to *Edit settings...*.
+
image:vmware-vcf-asa-image77.png[DPG - edit settings]
+
{nbsp}
. On *Distributed Port Group - Edit Settings* page, navigate to *Teaming and failover* in the left-hand menu and click on *uplink2* to move it down to *Unused uplinks*.
+
image:vmware-vcf-asa-image78.png[move uplink2 to unused]

. Repeat this step for the second iSCSI port group. However, this time move *uplink1* down to *Unused uplinks*.
+
image:vmware-vcf-asa-image79.png[move uplink 1 to unused]
====

.Create VMkernel adapters on each ESXi host
[%collapsible]
====
Repeat this process on each ESXi host in the Workload Domain.

. From the vSphere client navigate to one of the ESXi hosts in the Workload Domain inventory. From the *Configure* tab select *VMkernel adapters* and click on *Add Networking...* to start.
+
image:vmware-vcf-asa-image30.png[Start add networking wizard]
+
{nbsp}
. On the *Select connection type* window choose *VMkernel Network Adapter* and click on *Next* to continue.
+
image:vmware-vcf-asa-image08.png[Choose VMkernel Network Adapter]
+
{nbsp}
. On the *Select target device* page, choose one of the distributed port groups for iSCSI that was created previously.
+
image:vmware-vcf-asa-image95.png[Choose target port group]
+
{nbsp}
. On the *Port properties* page click the box for *NVMe over TCP* and click on *Next* to continue.
+
image:vmware-vcf-asa-image96.png[VMkernel port properties]
+
{nbsp}
. On the *IPv4 settings* page fill in the *IP address*, *Subnet mask*, and provide a new Gateway IP address (only if required). Click on *Next* to continue.
+ 
image:vmware-vcf-asa-image97.png[VMkernel IPv4 settings]
+
{nbsp}
. Review the your selections on the *Ready to complete* page and click on *Finish* to create the VMkernel adapter.
+
image:vmware-vcf-asa-image98.png[Review VMkernel selections]
+
{nbsp}
. Repeat this process to create a VMkernel adapter for the second iSCSI network.
====






For additional information:

For information on configuring ONTAP storage systems refer to the link:https://docs.netapp.com/us-en/ontap[ONTAP 9 Documentation] center.

For information on configuring VCF refer to link:https://docs.vmware.com/en/VMware-Cloud-Foundation/index.html[VMware Cloud Foundation Documentation].